{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# reformat distmat produced by raxml\n",
    "dist_file = \"../otu_data/tree_data/not_full_tree/RAxML_distances.query_high_abund.distmat\"\n",
    "melted_dists = pd.read_csv(dist_file, sep=\"\\t\", usecols=range(2), header=None)\n",
    "melted_dists.columns = ['OTUS', 'Distance']\n",
    "\n",
    "# split first column\n",
    "temp = melted_dists[\"OTUS\"].str.split(\" \", n = 1, expand = True) \n",
    "melted_dists[\"OTUONE\"]= temp[0] \n",
    "melted_dists[\"OTUTWO\"]= temp[1] \n",
    "melted_dists.drop(['OTUS'], inplace=True, axis=1)\n",
    "melted_srs = melted_dists.set_index(['OTUONE', 'OTUTWO']).Distance\n",
    "dist_df = melted_srs.unstack(level=-1)\n",
    "dist_df.columns = [i.strip() for i in dist_df.columns]\n",
    "\n",
    "# Make into a hollow symmetrical matrix\n",
    "dist_df.insert(0, 'OTU1', pd.Series(index=dist_df.index))\n",
    "dist_t = dist_df.T\n",
    "dist_df2 = dist_t.join(pd.Series(index=dist_t.index, name='OTU9982')).T\n",
    "distmat = dist_df2.values\n",
    "np.fill_diagonal(distmat, 0)\n",
    "dist_df3 = pd.DataFrame(distmat, index=dist_df2.index, columns=dist_df2.columns)\n",
    "upper_triangle = dist_df3.fillna(0).values\n",
    "symmetric_ = upper_triangle + upper_triangle.T\n",
    "\n",
    "# write out\n",
    "done_dist = pd.DataFrame(symmetric_, index=dist_df3.index, columns=dist_df3.columns)\n",
    "done_dist.to_csv(\"../otu_data/dispersal_selection_data/not_full_tree_distances.tsv\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this file is to load the ASV table, load and compare clustered sequences, check each for structural artifacts with a PCA paired with a Chi-Squared test, make some diagnostic figures, remove low frequency features, compare UNIFRAC-type and conventional distances between samples, and then make a hierarchy visualize the similarities between samples, and save the final output matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load packages and OTU abundance table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "import yaml\n",
    "\n",
    "# load raw abundance data \n",
    "abund_feather = '../otu_data/tree_data/hq_asv_table.feather'\n",
    "abund_tsv = \"../otu_data/tree_data/hq_asv_table.tsv\"\n",
    "if not os.path.exists(abund_feather):\n",
    "    abund_df = pd.read_csv(abund_tsv, sep=\"\\t\")\n",
    "    abund_df.to_feather(abund_feather)\n",
    "else:\n",
    "    abund_df = pd.read_feather(abund_feather, use_threads=True).set_index('Samples')\n",
    "    \n",
    "print(\"Read in abundance table with {} rows and {} columns\".format(abund_df.shape[0], abund_df.shape[1]))\n",
    "\n",
    "# load full sample sheet\n",
    "config_file = \"config.yml\"\n",
    "with open(config_file, 'r') as stream:\n",
    "    cfg_dict = yaml.safe_load(stream)\n",
    "\n",
    "data_dir = cfg_dict['data_directory']\n",
    "sample_sheet_fn = cfg_dict['sample_sheet']\n",
    "sample_sheet = pd.read_csv(sample_sheet_fn, sep=\"\\t\")\n",
    "print(\"Read in metadata table with {} rows and {} columns\".format(sample_sheet.shape[0], sample_sheet.shape[1]))\n",
    "\n",
    "# Fix weird date\n",
    "sample_sheet.loc[sample_sheet['DateMMDDYY'] == 'Mix9', 'DateMMDDYY'] = '100516'\n",
    "\n",
    "# make weird samples (not mine or controls, some not in sample sheet) their own group \n",
    "weird_samples = set(abund_df.index) - set(sample_sheet.SampleID.unique())\n",
    "abund_df_jm = abund_df.drop(weird_samples, axis=0)\n",
    "print(\"After removing others' samples abundance table size is {}\".format(abund_df_jm.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Prep the sample sheet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sheet2 = sample_sheet.set_index('SampleID')\n",
    "sub_sample_sheet = sample_sheet2.loc[abund_df_jm.index, :]\n",
    "sub_sample_sheet.loc[sub_sample_sheet.DepthName.isnull(), 'DepthName'] = 'LAB'\n",
    "sub_sample_sheet.loc[sub_sample_sheet.DepthName == 'Surface', 'DepthName'] = '1'\n",
    "sub_sample_sheet.loc[sub_sample_sheet.DepthName == '0', 'DepthName'] = '1'\n",
    "sub_sample_sheet.loc[[i for i in sub_sample_sheet.index if 'FiltCtrl' in i], 'StationName'] = 'CB44'\n",
    "select_metadata = ['DateMMDDYY', 'StationName', 'DepthName', 'sequencing ID']\n",
    "\n",
    "lat_lon_fn = \"../otu_data/CB_Locations.tsv\"\n",
    "lat_lon = pd.read_csv(lat_lon_fn, sep=\"\\t\", index_col=0)\n",
    "cb2lat, cb2lon = {'LAB': 00.0}, {'LAB': 00.0}\n",
    "\n",
    "for stat_ in lat_lon.index:\n",
    "    if stat_.replace(\".\", \"\") in sub_sample_sheet.StationName.unique():\n",
    "        cb2lat[stat_.replace(\".\", \"\")] = round(lat_lon.loc[stat_, 'Latitude'], 3) \n",
    "        cb2lon[stat_.replace(\".\", \"\")] = round(lat_lon.loc[stat_, 'Longitude'], 3) \n",
    "\n",
    "\n",
    "sub_sample_sheet['Latitude'] = sub_sample_sheet.StationName.map(cb2lat)\n",
    "sub_sample_sheet['Longitude'] = sub_sample_sheet.StationName.map(cb2lon)\n",
    "\n",
    "read_counts = pd.read_csv(\"../otu_data/trim_stats/read_counts.tsv\", sep=\"\\t\", header=None, index_col=0)\n",
    "sub_reads = read_counts.loc[sub_sample_sheet.index, :]\n",
    "sub_reads.columns = ['RawCount', 'TrimCount']\n",
    "\n",
    "read_bins = [5e3, 1e4, 2.5e4, 5e4, 1e5, 2.5e5, 5e5, 1e6, 2.5e6]\n",
    "read_discrete = pd.DataFrame(index=sub_reads.index, columns=['RawCount_b', 'TrimCount_b'],\n",
    "                            data=np.zeros(sub_reads.shape))\n",
    "for rb in read_bins:\n",
    "    r_bool = sub_reads['RawCount'] >= rb\n",
    "    read_discrete.loc[r_bool, 'RawCount_b'] += 1\n",
    "    t_bool = sub_reads['TrimCount'] >= rb\n",
    "    read_discrete.loc[t_bool, 'TrimCount_b'] += 1\n",
    "    print(\"{} and {} libraries incremented\".format(r_bool.sum(), t_bool.sum()))\n",
    "\n",
    "meta_data_df = sub_sample_sheet.join(sub_reads).join(read_discrete)\n",
    "\n",
    "md_df = meta_data_df.copy()\n",
    "odu_set = set(md_df[md_df['Short sample name'].str.contains(\"ODU\") | \n",
    "                    md_df[\"Sampling notes\"].str.contains(\"ODU\")].index)\n",
    "dnr_set = set(md_df[md_df['Short sample name'].str.contains(\"DNR\") | \n",
    "                    md_df[\"Sampling notes\"].str.contains(\"DNR\")].index)\n",
    "\n",
    "dnr_set.update([i for i in md_df.index if 'FiltCtrl' in i])\n",
    "print(len(odu_set), len(dnr_set), len(odu_set.intersection(dnr_set)))\n",
    "\n",
    "non_pl = odu_set.union(dnr_set)\n",
    "md_df_other = md_df.loc[~md_df.index.isin(non_pl), :]\n",
    "possibly_pl = set(md_df_other[md_df_other.StationName == 'CB33C'].index)\n",
    "\n",
    "print(len(possibly_pl), len(possibly_pl.intersection(odu_set)), len(possibly_pl.intersection(dnr_set)))\n",
    "\n",
    "meta_data_df = meta_data_df.join(pd.Series(index=meta_data_df.index, name=\"CollectionAgency\"))\n",
    "meta_data_df.loc[odu_set, 'CollectionAgency'] = 'ODU'\n",
    "meta_data_df.loc[dnr_set, 'CollectionAgency'] = 'DNR'\n",
    "meta_data_df.loc[possibly_pl, 'CollectionAgency'] = 'Preheim'\n",
    "\n",
    "print(meta_data_df.CollectionAgency.isnull().sum(), meta_data_df.CollectionAgency.shape)\n",
    "\n",
    "sid_map = {'esakows1_132789': 'e_13',\n",
    "           'controls': 'controls',\n",
    "           'esakows1_152133_plate_1': 'e_15_1',\n",
    "           'esakows1_152133_plate_2': 'e_15_2',\n",
    "           'Keith_Maeve1_138650': 'KM',\n",
    "           'Miseq_data_SarahPreheim_Sept2016': 'Miseq_sp',\n",
    "           'sprehei1_123382': 'spr12',\n",
    "           'sprehei1_149186': 'spr14'}\n",
    "\n",
    "print(meta_data_df[\"sequencing ID\"].unique())\n",
    "meta_data_df.loc[:, 'sequencing_ID'] = meta_data_df.loc[:, 'sequencing ID'].map(sid_map)\n",
    "select_metadata.remove(\"sequencing ID\"); select_metadata.append(\"sequencing_ID\");\n",
    "print(meta_data_df.loc[:, \"sequencing_ID\"].unique())\n",
    "\n",
    "control_libs = (['178A_WaterBathControlA', '178B_WaterBathControlB'])\n",
    "control_libs += list(abund_df_jm.index[abund_df_jm.index.str.contains(\"Blank\")])\n",
    "control_libs += list(abund_df_jm.index[abund_df_jm.index.str.contains(\"Mix9\")])\n",
    "control_libs += list(abund_df_jm.index[abund_df_jm.index.str.contains(\"CDSBBR\")])\n",
    "control_libs += list(abund_df_jm.index[abund_df_jm.index.str.contains(\"EMPTY\")])\n",
    "control_libs += list(abund_df_jm.index[meta_data_df['Short sample name'].str.contains(\"_Neg\")])\n",
    "control_libs += list(abund_df_jm.index[meta_data_df['Short sample name'].str.contains(\"ML0\")])\n",
    "control_libs += list(abund_df_jm.index[meta_data_df['Short sample name'].str.contains(\"_Pos\")])\n",
    "control_libs = list(set(control_libs))\n",
    "\n",
    "meta_data_df.loc[control_libs +[i for i in meta_data_df.index if 'Filt' in i], \"sequencing_ID\"] = 'controls'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we can see what OTUs present in control libraries are removed by different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrls = control_libs\n",
    "# start blanks\n",
    "start_blanks = abund_df_jm.index[abund_df_jm.index.str.contains(\"CDSBBR\")]\n",
    "# stevens samples\n",
    "stevens_samples = [i for i in weird_samples if 'SJC' in i]\n",
    "# bens samples\n",
    "bens_samples = [i for i in weird_samples if 'BF' in i]\n",
    "# andrea's samples\n",
    "andreas_samples = [i for i in weird_samples if 'ANF' in i]\n",
    "# and all experimental samples are my noncontrols\n",
    "exp_samps = abund_df_jm.index[~(abund_df_jm.index.isin(ctrls))]\n",
    "# emptys\n",
    "empty_index = abund_df_jm.index[abund_df_jm.index.str.contains(\"EMPTY\")]\n",
    "\n",
    "\"\"\"\n",
    "abund_table = abund_df.copy()\n",
    "abund_ra = abund_table.div(abund_table.sum(1), axis=0)\n",
    "\n",
    "# 50% of relative abundance in controls\n",
    "control_otus1 = abund_ra.columns[abund_ra.loc[ctrls, :].sum() > 0]\n",
    "total_in_controls1 = abund_ra.loc[ctrls, control_otus1].sum()\n",
    "total_in_non_controls1 = abund_ra.loc[~abund_ra.index.isin(ctrls), control_otus1].sum()\n",
    "mostly_in_controls1 = control_otus1[(total_in_controls1 / total_in_non_controls1) > 0.5]\n",
    "otus_to_strip1 = set(mostly_in_controls1)\n",
    "\n",
    "# 50% of read counts in controls\n",
    "control_otus2 = abund_table.columns[abund_table.loc[ctrls, :].sum() > 0]\n",
    "total_in_controls2 = abund_table.loc[ctrls, control_otus2].sum()\n",
    "total_in_non_controls2 = abund_table.loc[~abund_table.index.isin(ctrls), control_otus2].sum()\n",
    "mostly_in_controls2 = control_otus2[(total_in_controls2 / total_in_non_controls2) > 0.5]\n",
    "otus_to_strip2 = set(mostly_in_controls2)\n",
    "\n",
    "# median relative abundance greater in controls\n",
    "control_otus3 = abund_ra.columns[abund_ra.loc[ctrls, :].sum() > 0]\n",
    "total_in_controls3 = abund_ra.loc[ctrls, control_otus3].apply(np.mean)\n",
    "total_in_non_controls3 = abund_ra.loc[~abund_ra.index.isin(ctrls), control_otus3].apply(np.mean)\n",
    "mostly_in_controls3 = control_otus3[(total_in_controls3 >= total_in_non_controls3)]\n",
    "otus_to_strip3 = set(mostly_in_controls3)\n",
    "\n",
    "import seaborn as sns; import matplotlib.pyplot as plt;\n",
    "sns.set(style=\"white\", color_codes=True);\n",
    "to_plot = pd.concat((np.log(total_in_controls3), np.log(total_in_non_controls3)), axis=1)\n",
    "to_plot.replace([np.inf, -np.inf], np.nan).dropna(how='any', inplace=True)\n",
    "to_plot.columns = ['control mean', 'non-control mean']\n",
    "to_plot.loc[(to_plot['non-control mean'] == -np.inf), 'non-control mean'] = -18.0\n",
    "def col_switch(x):\n",
    "    if x in otus_to_strip3 and not x in otus_to_strip1:\n",
    "        return 1\n",
    "    elif x in otus_to_strip1 and x in otus_to_strip3:\n",
    "        return 3\n",
    "    elif not x in otus_to_strip1 and not x in otus_to_strip3:\n",
    "        return 0\n",
    "\n",
    "to_plot = to_plot.reset_index()\n",
    "to_plot['currentMethod'] = to_plot['index'].apply(col_switch)\n",
    "print(to_plot.isnull().sum())\n",
    "\n",
    "g = sns.jointplot(x=\"control mean\", y=\"non-control mean\", data=to_plot, kind=\"kde\", \n",
    "                  height=7, xlim=(-17,-3), ylim=(-20,-3))\n",
    "g.savefig(\"../otu_data/pca_plots/contamination_filters_nc.png\", dpi=150)\n",
    "\n",
    "g.ax_joint.collections[0].set_visible(False)\n",
    "\n",
    "for i in to_plot.index:\n",
    "    if to_plot.loc[i, 'currentMethod'] == 3:\n",
    "        g.ax_joint.plot(to_plot.loc[i, \"control mean\"], \n",
    "                        to_plot.loc[i, \"non-control mean\"], color='blue', marker='o', alpha=.6)\n",
    "    elif to_plot.loc[i, 'currentMethod'] == 1:\n",
    "        g.ax_joint.plot(to_plot.loc[i, \"control mean\"], \n",
    "                        to_plot.loc[i, \"non-control mean\"], color='red', marker='o', alpha=.6)\n",
    "    elif to_plot.loc[i, 'currentMethod'] == 0:\n",
    "        g.ax_joint.plot(to_plot.loc[i, \"control mean\"], \n",
    "                        to_plot.loc[i, \"non-control mean\"], color='green', marker='o', alpha=.6)\n",
    "\n",
    "g.set_axis_labels('log control mean', \"log non-control mean\", fontsize=16)\n",
    "g.savefig(\"../otu_data/pca_plots/contamination_filters.png\", dpi=150)\n",
    "to_plot.sort_values(by=\"non-control mean\", ascending=False).head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we will define a function that drops features present mostly in controls and samples with low yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrease_sparsity(abund_table, ctrls, abund_thresh=0.002, div_thresh=100, rare_thresh=3000, addl_keys=[]):\n",
    "    \"\"\"Takes an abundance table and a list of control indexes\n",
    "       Removes OTUs with 50% or more of their abundances in controls.\n",
    "       Removes OTUS below user set abundances threshold.\n",
    "       Removes features below rarefaction threshold.\n",
    "       Removes additional features according to string matched key\"\"\"\n",
    "    otus_to_strip = set()\n",
    "    abund_ra = abund_table.div(abund_table.sum(1), axis=0)\n",
    "    control_otus = abund_ra.columns[abund_ra.loc[ctrls, :].sum() > 0]\n",
    "    total_in_controls = abund_ra.loc[ctrls, control_otus].apply(np.mean)\n",
    "    total_in_non_controls = abund_ra.loc[~abund_ra.index.isin(ctrls), control_otus].apply(np.mean)\n",
    "    mostly_in_controls = control_otus[(total_in_controls >= total_in_non_controls)]\n",
    "    otus_to_strip.update(mostly_in_controls)\n",
    "    print(\"{} are to be removed as contaminants\".format(len(otus_to_strip)))\n",
    "    low_abund = abund_table.columns[(abund_ra > abund_thresh).sum() == 0]\n",
    "    otus_to_strip.update(low_abund)\n",
    "    print(\"{} are to be removed after adding low abundance OTUs\".format(len(otus_to_strip)))\n",
    "    presence_absence = ((abund_table > 0))\n",
    "    infrequent_appearances = abund_table.columns[presence_absence.sum() < 2]\n",
    "    otus_to_strip.update(infrequent_appearances)\n",
    "    print(\"{} are to be removed after adding low freq OTUs\".format(len(otus_to_strip)))\n",
    "    div_samples = presence_absence.sum(1)\n",
    "    print(\"Removing samples with fewer features than Zymo ({})\".format(div_thresh))\n",
    "    abund_to_return = abund_table.copy()\n",
    "    for ak in addl_keys: \n",
    "        ctrls += list(abund_table.index[abund_table.index.str.contains(ak)])\n",
    "    \n",
    "    ctrls += list(abund_table.index[div_samples < div_thresh])\n",
    "    ctrls += list(abund_table.index[abund_to_return.sum(1) < rare_thresh])\n",
    "    \n",
    "    fmt_libs = \"\".join([\"\\t\"+cl+\"\\n\" for cl in set(ctrls)])\n",
    "    print(\"Removed libraries are:\\n{}\".format(fmt_libs))\n",
    "        \n",
    "    abund_to_return = abund_to_return.loc[~(abund_table.index.isin(ctrls)), \n",
    "                                          ~abund_table.columns.isin(otus_to_strip)]\n",
    "    \n",
    "    print(\"{}, {} are shapes after bad otus and low yeild samples\".format(abund_table.shape, abund_to_return.shape))\n",
    "    return abund_to_return\n",
    "\n",
    "print(\"Decreasing sparsity of full table:\")\n",
    "abund_df_og_s1 = decrease_sparsity(abund_df_jm.copy(), control_libs, abund_thresh=0.002, addl_keys=['Zymo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More metadata prep work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_df = pd.concat([abund_df_og_s1, meta_data_df.loc[abund_df_og_s1.index, :]], axis=1)\n",
    "super_df.DepthName = super_df.DepthName.apply(lambda x: \"0\"+x if len(x) == 1 else x)\n",
    "super_df['Month'] = super_df.DateMMDDYY.apply(lambda x: x[:2])\n",
    "super_df['Year'] = super_df.DateMMDDYY.apply(lambda x: x[-2:])\n",
    "super_df['Month_Year'] = super_df.loc[:, ['Month', 'Year']].apply(lambda x: \" \".join(x), axis=1)\n",
    "sort_vars = ['Year', 'Month', 'Latitude', 'DepthName']\n",
    "super_df.sort_values(sort_vars, ascending=[True, True, False, True], inplace=True)\n",
    "super_df['Salinity_Group'] = pd.Series([\"\"]*super_df.index.shape[0], index=super_df.index)\n",
    "some_indexes = super_df.index[[0,1,2,100,101,102,200,201,202,-3,-2,-1]]\n",
    "some_columns = ['DepthName', 'Month', 'Year', 'Longitude', 'CollectionAgency', 'Salinity_Group', 'sequencing_ID']\n",
    "super_df.loc[some_indexes, some_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write out a table of dates, depths, and stations for collaborators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_surf = [i for i in super_df.index if 'CD0BR' in i]\n",
    "super_df.loc[true_surf, 'DepthName'] = '00'\n",
    "some_columns = ['DepthName', 'DateMMDDYY', 'Month', 'Year', 'StationName', 'CollectionAgency']\n",
    "to_remove = super_df.DepthName.isin(['LAB', 'lab'])\n",
    "subdf = super_df.loc[~to_remove, some_columns].drop_duplicates()\n",
    "subdf.to_csv(\"../otu_data/Preheim_CB_DepthStationYear.tab\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in water quality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_file = \"../otu_data/WaterQualityData/melted_wq_07to17.tsv\"\n",
    "if not os.path.exists(melted_file):\n",
    "    db_file = \"../otu_data/WaterQualityData/WaterQualityData_07to17.tsv\"\n",
    "    df_raw = pd.read_csv(db_file, sep=\"\\t\", parse_dates=[['SampleDate', 'SampleTime']], low_memory=False)\n",
    "    df_nn = df_raw[df_raw.MeasureValue.notnull()]\n",
    "    df_nn['SampleDate_SampleTime'] = df_nn.SampleDate_SampleTime.apply(pd.to_datetime)\n",
    "    summer_idxs = set()\n",
    "    for yr_i, yr in enumerate(range(2012,2018)):\n",
    "        left_side = pd.to_datetime(str(yr)+'-03-15')\n",
    "        right_side = pd.to_datetime(str(yr)+'-10-15')\n",
    "        date_range_ = (df_nn.SampleDate_SampleTime >= left_side) & (df_nn.SampleDate_SampleTime < right_side)\n",
    "        summer_idxs.update(df_nn[date_range_].index)\n",
    "        print(\"Window {} is {} to {}, grabbed {} total\".format(yr_i, left_side, right_side, len(summer_idxs)))\n",
    "\n",
    "    df_nv = df_nn.loc[summer_idxs, :]\n",
    "    df_nv.loc[df_nv.Problem == 'NV', 'Problem'] = np.nan\n",
    "    df_nv.loc[df_nv.Problem == 'QQ', 'Problem'] = np.nan\n",
    "    df_nv.loc[df_nv.Problem == 'WW', 'Problem'] = np.nan\n",
    "    df_np = df_nv[df_nv.Problem.isnull()]\n",
    "\n",
    "    needed_cols = ['TotalDepth', 'UpperPycnocline', 'LowerPycnocline']\n",
    "    idx_cols = [\"SampleDate_SampleTime\", \"Station\", \"Depth\"]\n",
    "    new_df = pd.pivot_table(data = df_np, index = idx_cols, columns = 'Parameter', \n",
    "                            values = 'MeasureValue', aggfunc=np.mean).sort_index()\n",
    "\n",
    "    addl_cols = df_np.loc[:, idx_cols+needed_cols].groupby(idx_cols).agg(np.mean).sort_index()\n",
    "    wq_melted = new_df.join(addl_cols)\n",
    "    wq_melted.to_csv(melted_file, sep=\"\\t\")\n",
    "else:\n",
    "    print(\"Reading melted water quality data from file\")\n",
    "    wq_melted_df = pd.read_csv(melted_file, sep=\"\\t\")\n",
    "    print(\"Df size {}\".format(wq_melted_df.shape))\n",
    "    print(\"First index: \", wq_melted_df.iloc[0, :3].values, \"\\nLast Index: \", wq_melted_df.iloc[-1, :3].values)\n",
    "    print(wq_melted_df.isnull().sum().sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get actual pycnocline locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exo_dir = \"../otu_data/WaterQualityData/PreheimProbeData\"\n",
    "exo_files = sorted([i for i in os.listdir(exo_dir) if i.endswith(\".xlsx\")])\n",
    "print(exo_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date1(x):\n",
    "    a = [float(i) for i in x.split()[0].split(\"-\")]\n",
    "    return a[0] + a[1]/12 + a[2]/(365.3333)\n",
    "\n",
    "def date2(a):\n",
    "    return float(a[:2])/(12) + float(a[2:4])/(365.3333) + (float(a[4:])+2000)\n",
    "\n",
    "super_matched = super_df.copy()[super_df.DepthName != 'LAB']\n",
    "wq_melted_df['date_float'] = pd.to_datetime(wq_melted_df['SampleDate_SampleTime']).astype(np.int64) // 10**9\n",
    "super_matched['date_float'] = pd.to_datetime(super_matched['DateMMDDYY'], format=\"%m%d%y\").astype(np.int64) // 10**9\n",
    "super_matched['SpaceTime'] = super_matched.loc[:, ['DateMMDDYY', 'date_float', 'StationName']].apply(tuple, axis=1)\n",
    "\n",
    "isint = lambda x: True if x%1.0 == 0 else False\n",
    "water_nzd = wq_melted_df[(wq_melted_df.Depth != 0) & wq_melted_df.Depth.apply(isint)]\n",
    "\n",
    "idx_names = ['DateMMDDYY', 'date_float', 'StationName']\n",
    "wqm_idx = pd.MultiIndex.from_tuples([tuple(i) for i in super_matched.SpaceTime.unique()], names=idx_names)\n",
    "wqm_df = pd.DataFrame(index=wqm_idx, columns=['DensityMatchDate', 'DensityTimeDelta',\n",
    "                                              'PycDepth', 'BVF', 'FoldChange']).sort_index()\n",
    "\n",
    "density_profiles = []\n",
    "\n",
    "for sd_x in wqm_df.index:\n",
    "    mdy_t, ux_t, stat_ = sd_x\n",
    "    stat_mod = stat_[:3] +\".\" + stat_[3:5]\n",
    "    stat_select = water_nzd[water_nzd.Station == stat_mod]\n",
    "    \n",
    "    print(stat_mod, mdy_t, stat_select.shape)\n",
    "    donebool = False\n",
    "    timediff = abs(stat_select.date_float - ux_t) / (24*60**2)\n",
    "    while not donebool:\n",
    "        matched_time, time_delt = timediff.idxmin(), timediff.min()\n",
    "        selected_time = stat_select.loc[matched_time, 'SampleDate_SampleTime']\n",
    "        time_select = stat_select[stat_select.SampleDate_SampleTime == selected_time]\n",
    "        #print(selected_time, mdy_t, time_select.shape)\n",
    "        select_densities = time_select.sort_values(['Depth']).loc[:, ['Depth', 'SIGMA_T']]\n",
    "        depth_gradient = np.gradient(select_densities.SIGMA_T.values, select_densities.Depth.values)\n",
    "        select_densities['BVF'] = pd.Series(depth_gradient, index=select_densities.index)\n",
    "        select_densities = select_densities[select_densities.Depth > 2.0]\n",
    "        if select_densities.BVF.isnull().sum() > 2:\n",
    "            print(\"bad date\")\n",
    "            donebool = True\n",
    "            timediff = timediff[timediff > time_delt]\n",
    "        else:\n",
    "            donebool = True\n",
    "            wqm_df.loc[sd_x, 'PycDepth'] = select_densities.loc[select_densities['BVF'].idxmax(), 'Depth']\n",
    "            wqm_df.loc[sd_x, 'BVF'] = select_densities.loc[select_densities['BVF'].idxmax(), 'BVF']\n",
    "            wqm_df.loc[sd_x, 'FoldChange'] = wqm_df.loc[sd_x, 'BVF']/np.median(select_densities['BVF'])\n",
    "            wqm_df.loc[sd_x, 'DensityMatchDate'] = selected_time\n",
    "            wqm_df.loc[sd_x, 'DensityTimeDelta'] = time_delt\n",
    "            density_profiles.append(select_densities.copy())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# maybe make date distance threshold harsher\n",
    "# get precipitation and windspeed data to confirm\n",
    "# should make options for upper/lower water column w/o stratification\n",
    "\"\"\"\n",
    "\n",
    "for new_var in ['Pycnocline', 'dSigmaT_dDepth', 'PycDepth']:\n",
    "    super_matched[new_var] = pd.Series([np.nan]*super_matched.shape[0], index=super_matched.index)\n",
    "\n",
    "wqm_df = wqm_df[wqm_df.DensityTimeDelta < 5]\n",
    "wqm_df = wqm_df[wqm_df.FoldChange.notnull()]\n",
    "wqm_df = wqm_df[wqm_df.FoldChange > 3]\n",
    "\n",
    "\n",
    "for dat_MDY, date_flt, statn in wqm_df.index:\n",
    "    #print(tuple([dat_MDY, date_flt, statn]))\n",
    "    this_depth = wqm_df.PycDepth[tuple([dat_MDY, date_flt, statn])]\n",
    "    this_bvf = wqm_df.BVF[tuple([dat_MDY, date_flt, statn])]\n",
    "    dat_bool = super_matched['DateMMDDYY'] == dat_MDY\n",
    "    dflt_bool = super_matched['date_float'] == date_flt \n",
    "    stat_bool = super_matched['StationName'] == statn\n",
    "    above_bool = super_matched['DepthName'].astype(int) <= this_depth\n",
    "    below_bool = super_matched['DepthName'].astype(int) > this_depth\n",
    "    super_matched.loc[dat_bool & dflt_bool & stat_bool & above_bool, 'Pycnocline'] = 'Above'\n",
    "    super_matched.loc[dat_bool & dflt_bool & stat_bool & below_bool, 'Pycnocline'] = 'Below'\n",
    "    super_matched.loc[dat_bool & dflt_bool & stat_bool, 'PycDepth'] = this_depth\n",
    "    super_matched.loc[dat_bool & dflt_bool & stat_bool, 'dSigmaT_dDepth'] = this_bvf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at results of density gradient calculations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wqm_df_full = wqm_df.reset_index()\n",
    "wqm_df_full['FoldChange'] = wqm_df_full['FoldChange'].astype(float)\n",
    "wqm_df_full['StatEncoded'] = wqm_df_full.loc[:, 'StationName'].map(metas_ls['encoding']['StationName'])\n",
    "wqm_df_full['DatEncoded'] = wqm_df_full.loc[:, 'DateMMDDYY'].map(metas_ls['encoding']['DateMMDDYY'])\n",
    "\n",
    "wqm_pivot = pd.pivot_table(wqm_df_full, columns=['StatEncoded'], values=['FoldChange'], \n",
    "                           index=['DatEncoded'], fill_value=-1.)\n",
    "\n",
    "mask = np.zeros_like(wqm_pivot.values)\n",
    "mask[wqm_pivot.values == -1.] = True\n",
    "fc_fig, fc_ax = plt.subplots(nrows=1, ncols=1, figsize=(6,6), dpi=120)\n",
    "ax_fc = sns.heatmap(wqm_pivot.values*-1, mask=mask, vmin=-30, vmax=4, ax=fc_ax)\n",
    "ax_fc.set_title('Fold Change in Brunt–Väisälä Frequency at Pycnocline', fontsize=12)\n",
    "ax_fc.set(xlabel='Dates', ylabel='Stations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import taxa table and print some random rows to take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxa_file = \"taxa_table.tsv\"\n",
    "data_path = \"../otu_data/dada2_outputs\"\n",
    "tax_f = os.path.join(data_path, taxa_file)\n",
    "taxa_df = pd.read_csv(tax_f, sep=\"\\t\")\n",
    "OTU_Seqs = {taxa_df.loc[idx, taxa_df.columns[0]]:idx for idx in taxa_df.index}\n",
    "OTU_Names = {idx:\"OTU{}\".format(idx+1) for idx in taxa_df.index }\n",
    "OTU_name2seq = {OTU_Names[num]:seq for seq, num in OTU_Seqs.items()}\n",
    "taxa_df.loc[:, taxa_df.columns[0]] = taxa_df.loc[:, taxa_df.columns[0]].apply(lambda x: OTU_Names[OTU_Seqs[x]])\n",
    "taxa_df = taxa_df.set_index(taxa_df.columns[0])\n",
    "taxa_df.loc[['OTU2880', 'OTU14561', 'OTU38271', \"OTU54666\", 'OTU15257', 'OTU30263', 'OTU2'], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is where we rarefy to 3001 total counts and look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skbio.stats import subsample_counts\n",
    "from skbio.diversity import beta_diversity\n",
    "import seaborn as sns\n",
    "from skbio.stats.distance import mantel\n",
    "\n",
    "def rarefy_table(abund_table, rare_level):\n",
    "    rare_abund = abund_table.copy() * 0.\n",
    "    total_abunds = abund_table.sum(1)\n",
    "    for samp in abund_table.index:\n",
    "        norm_factor = 1.0\n",
    "        if total_abunds[samp] < rare_level:\n",
    "            norm_factor = rare_level / total_abunds[samp]\n",
    "\n",
    "        select_vect = np.ceil(abund_table.loc[samp, :].values*norm_factor).astype(np.int64)\n",
    "        rare_vect = subsample_counts(select_vect, int(rare_level))\n",
    "        rare_abund.loc[samp, :] = rare_vect\n",
    "    return rare_abund\n",
    "\n",
    "disttabs = {}\n",
    "abundance_tables = {'Abundance Thresholded':abund_df_og_s1.copy()}\n",
    "for name_ab, ab_df in abundance_tables.items():\n",
    "    rare_pp = rarefy_table(ab_df, 3001)\n",
    "    sample_sums = ab_df.sum(1)\n",
    "    big_samples = ab_df[sample_sums > 300000].index\n",
    "    smaller_samples = ab_df[sample_sums < 10000].index\n",
    "    print(name_ab)\n",
    "    print(\"Big samples: {}, Small: {}\".format(len(big_samples), len(smaller_samples)))\n",
    "    plt.clf(); plt.close();\n",
    "    fixx, axx = plt.subplots(nrows=1, ncols=1, figsize=(6,6), dpi=130)\n",
    "    cols = ['gold', 'teal']\n",
    "    labs = ['full', 'rare']\n",
    "    for ixx, a_table in enumerate([ab_df, rare_pp]):\n",
    "        bc_dists = beta_diversity(\"braycurtis\", a_table.values, a_table.index)\n",
    "        disttabs[name_ab + \" \" + labs[ixx]] = bc_dists\n",
    "        bc_df = pd.DataFrame(index=a_table.index, columns=a_table.index, data=bc_dists._data)\n",
    "        print(np.median(bc_df.loc[big_samples, big_samples]), \"BvB\")\n",
    "        print(np.median(bc_df.loc[smaller_samples, smaller_samples]), \"SvS\")\n",
    "        print(np.median(bc_df.loc[big_samples, smaller_samples]), \"BvS\")\n",
    "        sns.distplot(bc_df.values.flatten(), axlabel='bray-curtis distance',\n",
    "                     kde_kws={\"label\":labs[ixx], \"lw\": 3}, ax=axx)\n",
    "    fixx.savefig(\"../otu_data/trim_stats/rarefaction_effect.png\")\n",
    "    plt.show()\n",
    "\n",
    "all_mat_names = list(disttabs.keys())\n",
    "for ix in range(len(all_mat_names)):\n",
    "    for jx in range(ix+1, len(all_mat_names)):\n",
    "        print(all_mat_names[ix], all_mat_names[jx])\n",
    "        r_pp, p_value_pp, n_pp = mantel(disttabs[all_mat_names[ix]], disttabs[all_mat_names[jx]], method='pearson')\n",
    "        print(r_pp, p_value_pp, n_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write out sequences to make a tree. This is commended out because it was done and should not be redone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_path = \"../otu_data/tree_data\"\n",
    "#fnames = ['query_high_abund002.fasta', 'query_high_abund0005.fasta']\n",
    "#abund_df_ogs = [abund_df_og_s1 ]#, abund_df_og_p1]\n",
    "#for fname, abund_df_og in zip(fnames, abund_df_ogs):\n",
    "#    heads = sorted(list(abund_df_og.columns))\n",
    "#    tails = [OTU_name2seq[i] for i in heads]\n",
    "#    with open(os.path.join(out_path, fname), \"w\") as wofh:\n",
    "#        print(wofh.write(\"\".join([\">{}\\n{}\\n\".format(i, j) for i, j in zip(heads, tails)])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This calculates the inter-run distances, plots their pdfs, and performs Mann-Whitney U test on each pairing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "groups = list(super_df['sequencing_ID'].unique())\n",
    "\n",
    "def calculate_group_distance(a_label, a_table):\n",
    "    # subset according to \"in\" label \n",
    "    in_indexes = super_df[super_df['sequencing_ID'] == a_label].index\n",
    "    \n",
    "    # subset according to \"out\" label\n",
    "    out_group = set(groups) - set([a_label]) \n",
    "    out_indexes = super_df[super_df['sequencing_ID'].isin(out_group)].index\n",
    "    assert len(out_indexes) + len(in_indexes) == len(a_table.index)\n",
    "    \n",
    "    # calculate bray-curtis for all groups\n",
    "    bc_dists = beta_diversity(\"braycurtis\", a_table.values, a_table.index)\n",
    "    bc_df = pd.DataFrame(bc_dists._data, index=a_table.index, columns=a_table.index)\n",
    "    \n",
    "    # subset rows by \"in\" and columns by \"out\" \n",
    "    sub_bcdf = bc_df.loc[in_indexes, out_indexes]\n",
    "    if sub_bcdf.shape[0] == 1:\n",
    "        flat_df = sub_bcdf.T\n",
    "        flat_df.columns = [a_label]\n",
    "    else:\n",
    "        flat_df = pd.DataFrame(sub_bcdf.values.flatten(), columns=[a_label])\n",
    "        \n",
    "    print(a_label, flat_df.shape, sub_bcdf.shape)\n",
    "    return (flat_df, flat_df[a_label].mean())\n",
    "\n",
    "out_data = {}\n",
    "for a_label in groups:\n",
    "    out_data[a_label] = calculate_group_distance(a_label, rare_pp.copy())\n",
    "\n",
    "_ = out_data.pop('controls'); groups.remove('controls'); \n",
    "\n",
    "flat_dfs = pd.DataFrame({l:fdf[l].sample(4000).values for l, (fdf, m) in out_data.items()})\n",
    "\n",
    "# calculate average distance \n",
    "\n",
    "color_choices = [\"sky blue\", \"olive\", \"gold\", \"teal\", \"rich blue\", \"wisteria\", \"lipstick red\"]\n",
    "\n",
    "plt.clf()\n",
    "f, _axes_ = plt.subplots(nrows=1, ncols=1, figsize=(12, 12), dpi=150)\n",
    "f.suptitle(\"Samples of all dist hists\")\n",
    "for a_label, cc in zip(groups, color_choices):\n",
    "    sns.distplot(flat_dfs[a_label], hist=False, axlabel='bray-curtis distance',\n",
    "                 kde_kws={\"label\":a_label, \n",
    "                          \"lw\": 3, \n",
    "                          \"color\": sns.xkcd_rgb[cc]}, \n",
    "                 ax=_axes_)\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"../otu_data/pca_plots/cross_dists_abthrsh.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.close()\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "pair_tests = pd.DataFrame(index=groups, columns=groups)\n",
    "for ix in pair_tests.index:\n",
    "    for cx in pair_tests.columns:\n",
    "        result = mannwhitneyu(flat_dfs.loc[:, cx].values, flat_dfs.loc[:, ix].values)\n",
    "        pair_tests.loc[ix, cx] = result[1]*2\n",
    "\n",
    "sig_level = 0.05/(7*6)\n",
    "print(\"Significance level is {}\".format(sig_level))\n",
    "pair_tests < sig_level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This creates a plot to show how many OTUs are shared across sequencing runs from the main station (CB33C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlycb33 = super_df['StationName'] == 'CB33C'\n",
    "adf = rare_pp.loc[onlycb33, :]\n",
    "mdf = super_df.loc[onlycb33, ~super_df.columns.isin(abund_df_og_s1.columns)]\n",
    "runs_to_keep = list(super_df['sequencing_ID'].unique())\n",
    "if 'controls' in runs_to_keep:\n",
    "    runs_to_keep.remove('controls')\n",
    "print(adf.shape, mdf.shape, runs_to_keep)\n",
    "shared_otus = pd.DataFrame(index=runs_to_keep, columns=runs_to_keep)\n",
    "shared_group = set(adf.columns)\n",
    "\n",
    "for run_grp1 in shared_otus.columns:\n",
    "    rg1_bool = mdf.sequencing_ID == run_grp1\n",
    "    print(\"{} has {} libraries\".format(run_grp1, rg1_bool.sum()))\n",
    "    for run_grp2 in shared_otus.index:\n",
    "        rg2_bool = mdf.sequencing_ID == run_grp2\n",
    "        otus_in_1 = adf.loc[rg1_bool, :].sum() / rg1_bool.sum()\n",
    "        otus_in_2 = adf.loc[rg2_bool, :].sum() / rg2_bool.sum()\n",
    "        foldchange = otus_in_1 / otus_in_2\n",
    "        num_shared = ((foldchange < 1000) & (foldchange > 0.001)).sum()\n",
    "        shared_otus.loc[run_grp2, run_grp1] = num_shared\n",
    "        # optional shared across all groups\n",
    "        shared_here = foldchange.index[(foldchange < 1000) & (foldchange > 0.001)]\n",
    "        shared_group = shared_group.intersection(set(shared_here))\n",
    "\n",
    "print(\"{} otus shared across all runs\".format(len(shared_group)))\n",
    "\n",
    "\n",
    "plt.clf(); plt.close();\n",
    "f, _axes_ = plt.subplots(nrows=1, ncols=1, figsize=(6, 6), dpi=125)\n",
    "sns.heatmap(shared_otus, annot=True, annot_kws={'fontsize':12}, fmt=\"d\", linewidths=.5, ax=_axes_, cbar=False)\n",
    "plt.savefig(\"../otu_data/OTU_Overlap_CB33_byRun_abthrsh.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.clf(); plt.close();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Alpha and Beta-Diversity Distances to Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skbio import TreeNode\n",
    "from io import StringIO\n",
    "from skbio.diversity import alpha_diversity\n",
    "import skbio.stats.composition as ssc \n",
    "from deicode.preprocessing import rclr\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "rare_fn = \"../otu_data/final_rarefied_table.tsv\"\n",
    "meta_fn = \"../otu_data/final_metadata.tsv\"\n",
    "if os.path.exists(rare_fn) and os.path.exists(meta_fn):\n",
    "    meta_df_x = pd.read_csv(meta_fn, sep=\"\\t\", index_col=0)\n",
    "    rare_abund = pd.read_csv(rare_fn, sep=\"\\t\", index_col=0)\n",
    "    print(rare_abund.shape, meta_df_x.shape)\n",
    "else:\n",
    "    tree_file = \"../otu_data/tree_data/not_full_tree/RAxML_rootedTree.root.query_high_abund.ref.tre\"\n",
    "    with open(tree_file, 'r') as tfh:\n",
    "        tree_str = tfh.read().strip()\n",
    "\n",
    "    tree_obj = TreeNode.read(StringIO(tree_str))\n",
    "\n",
    "    rare_pp_nz = rare_pp.loc[:, rare_pp.columns[rare_pp.sum() > 0]]\n",
    "    \n",
    "\n",
    "    print(\"Dropped {} empty columns\".format(rare_pp.shape[1] - rare_pp_nz.shape[1]))\n",
    "    print(\"Calculating Beta Diversity\")\n",
    "    wu_dm = beta_diversity(\"weighted_unifrac\", rare_pp_nz.values, \n",
    "                           list(rare_pp_nz.index), tree=tree_obj, \n",
    "                           otu_ids=list(rare_pp_nz.columns))._data\n",
    "    print(\"Finished unifrac\", wu_dm.shape)\n",
    "\n",
    "    bc_dm = beta_diversity(\"braycurtis\", rare_pp_nz.values, rare_pp_nz.index)._data\n",
    "    print(\"Finished bray curtis\", bc_dm.shape)\n",
    "\n",
    "    # clr transform\n",
    "    close_mat = ssc.closure(rare_pp_nz.values)\n",
    "    nz_mat = ssc.multiplicative_replacement(close_mat)\n",
    "    rclr_mat = rclr().fit_transform(nz_mat)\n",
    "    clr_dm = squareform(pdist(rclr_mat, 'euclidean'))\n",
    "    print(\"Finished clr+euclidean\", clr_dm.shape)\n",
    "\n",
    "    # put it all together \n",
    "    beta_mats = []\n",
    "    for suff, mat in zip(['_wu', '_bc', '_clr'], [wu_dm, bc_dm, clr_dm]):\n",
    "        new_cols = [i+suff for i in list(rare_pp_nz.index)]\n",
    "        dm_df = pd.DataFrame(mat, index=rare_pp_nz.index, columns=new_cols)\n",
    "        beta_mats.append(dm_df.copy())\n",
    "\n",
    "    super_beta = beta_mats[0].join(beta_mats[1]).join(beta_mats[2])\n",
    "    print(super_beta.shape)\n",
    "    \n",
    "    print(\"Calculating alpha diversity metrics and rarefaction table\")\n",
    "    alpha_cols = ['enspie', 'enspie_25', 'enspie_975', \n",
    "                  'observed_otus', 'observed_otus_25', 'observed_otus_975',\n",
    "                  'faith_pd', 'faith_pd_25', 'faith_pd_975']\n",
    "    \n",
    "    total_abunds = abund_df_og_s1.sum(1)\n",
    "    rare_level = 3001\n",
    "    bstrap_cnt = 100\n",
    "    enspies_bsts = pd.DataFrame(index=abund_df_og_s1.index, \n",
    "                                columns=range(bstrap_cnt)).astype(float)\n",
    "    obs_otuses_bsts = pd.DataFrame(index=abund_df_og_s1.index, \n",
    "                                   columns=range(bstrap_cnt)).astype(float)\n",
    "    faith_pd_bsts = pd.DataFrame(index=abund_df_og_s1.index, \n",
    "                                 columns=range(bstrap_cnt)).astype(float)\n",
    "    for r_ix in range(bstrap_cnt):\n",
    "        rare_table = rarefy_table(abund_df_og_s1.copy(), rare_level)\n",
    "        faith_pd_bsts.loc[:, r_ix] = alpha_diversity('faith_pd', rare_table.values, ids=list(rare_table.index), \n",
    "                                                     otu_ids=list(rare_table.columns), tree=tree_obj)\n",
    "        obs_otuses_bsts.loc[:, r_ix] = alpha_diversity('observed_otus', \n",
    "                                                       rare_table.values, \n",
    "                                                       list(rare_table.index))\n",
    "        enspies_bsts.loc[:, r_ix] = alpha_diversity('enspie', rare_table.values, list(rare_table.index))\n",
    "        print(\"Alpha calcs completion percent {:.2%}\".format((r_ix+1)/bstrap_cnt), sep=' ', end=\"\\r\", flush=True)\n",
    "    \n",
    "    alpha_metrics = pd.DataFrame(index=abund_df_og_s1.index, columns=alpha_cols).astype(float)\n",
    "    alpha_iterator = zip(['enspie', \"observed_otus\", 'faith_pd'], [enspies_bsts, obs_otuses_bsts, faith_pd_bsts])\n",
    "    pct_fxn = lambda x: np.percentile(x, [2.5, 50, 97.5])\n",
    "    for met_name, met_df in alpha_iterator:\n",
    "        met_srs = met_df.apply(pct_fxn, axis=1)\n",
    "        alpha_metrics.loc[met_srs.index, met_name] = met_srs.apply(lambda x: x[1])\n",
    "        alpha_metrics.loc[met_srs.index, met_name+'_25'] = met_srs.apply(lambda x: x[1] - x[0]) \n",
    "        alpha_metrics.loc[met_srs.index, met_name+'_975'] = met_srs.apply(lambda x: x[2] - x[1])\n",
    "    \n",
    "    m_data_cols_ = set(super_df.columns) - set(abund_df_og_s1.columns)\n",
    "    print(alpha_metrics.shape, super_beta.shape, )\n",
    "    meta_df_x = super_df.loc[:, m_data_cols_].join(alpha_metrics).join(super_beta)\n",
    "    rare_abund = rare_pp_nz.copy()\n",
    "    meta_df_x.to_csv(meta_fn, sep=\"\\t\")\n",
    "    rare_abund.to_csv(rare_fn, sep=\"\\t\")\n",
    "    print(\"Writing rarefied counts ({1}) and metadata ({0}) to file\".format(rare_abund.shape, meta_df_x.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add all the rest of the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix some station mislabellings back \n",
    "abund_md_df = rare_abund.join(meta_df_x)\n",
    "true_surf = [i for i in abund_md_df.index if 'CD0BR' in i]\n",
    "\n",
    "abund_md_df.loc[true_surf, 'DepthName'] = '00'\n",
    "\n",
    "# add salinity group\n",
    "sal_dict = {\"Oligohaline\": ['CB22', 'CB31'],\n",
    "            \"Mesohaline\": ['CB32', 'CB33C', 'CB41C', 'CB42C', 'CB43C', 'CB44', 'CB51', 'CB52', 'CB53', 'CB54'],\n",
    "            \"Polyhaline\": ['CB61', 'CB62', 'CB63', 'CB64', 'CB71', 'CB72', 'CB73', 'CB74']}\n",
    "\n",
    "for sal_i, stat_ls in sal_dict.items():\n",
    "    abund_md_df.loc[abund_md_df.StationName.isin(stat_ls), 'Salinity_Group'] = sal_i\n",
    "\n",
    "# add replicate column\n",
    "abund_md_df['replicate'] = pd.Series(pd.Categorical([1]*abund_md_df.shape[0], categories=[1,2,3]), \n",
    "                                     index=abund_md_df.index)\n",
    "\n",
    "st_cols = ['DateMMDDYY', 'DepthName', 'StationName']\n",
    "abund_md_df.loc[:, 'SpaceTime'] = abund_md_df.loc[:, st_cols].apply(tuple, axis=1)\n",
    "unq_tds = abund_md_df.SpaceTime.unique()\n",
    "print(\"{} potential replicates\".format(abund_md_df.shape[0] - unq_tds.shape[0]))\n",
    "\n",
    "dist_col_pairs = {\"wu\":[], \"bc\":[], 'clr':[]}\n",
    "for (t_, d_, s_) in abund_md_df.SpaceTime.unique():\n",
    "    mxO = abund_md_df[abund_md_df['SpaceTime'] == (t_, d_, s_)].index\n",
    "    if len(mxO) != 1:\n",
    "        pert_cols = [i for i in abund_md_df.columns if i.split(\"_\")[0] in list(mxO)]\n",
    "#        sub_dists = abund_md_df.loc[mxO, pert_cols].values.flatten()\n",
    "#        if (sub_dists > 0.3).sum() > 0:\n",
    "#            print(abund_md_df.loc[mxO, some_columns])\n",
    "#            print(abund_md_df.loc[mxO, pert_cols].values)\n",
    "        for num_r, ix_ in enumerate(mxO):\n",
    "            for pc in pert_cols:\n",
    "                if not ix_ in pc:\n",
    "                    if \"_bc\" in pc:\n",
    "                        dist_col_pairs['bc'].append((ix_, pc))\n",
    "                    elif \"_wu\" in pc:\n",
    "                        dist_col_pairs['wu'].append((ix_, pc))\n",
    "                    elif \"_clr\" in pc:\n",
    "                        dist_col_pairs['clr'].append((ix_, pc))\n",
    "            \n",
    "            abund_md_df.loc[ix_, 'replicate'] = num_r+1 \n",
    "\n",
    "bc_rep_vals = np.array([abund_md_df.loc[i, j] for i, j in dist_col_pairs['bc']], dtype=float)\n",
    "wu_rep_vals = np.array([abund_md_df.loc[i, j] for i, j in dist_col_pairs['wu']], dtype=float)\n",
    "clr_rep_vals = np.array([abund_md_df.loc[i, j] for i, j in dist_col_pairs['clr']], dtype=float)\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "rep_dists = np.hstack((minmax_scale(bc_rep_vals), \n",
    "                       minmax_scale(wu_rep_vals), \n",
    "                       minmax_scale(clr_rep_vals)))\n",
    "dist_row_names = ['bray-curtis']*bc_rep_vals.shape[0]\n",
    "dist_row_names += ['weighted-unifrac']*wu_rep_vals.shape[0]\n",
    "dist_row_names += ['clr+euclidean']*clr_rep_vals.shape[0]\n",
    "rep_names = np.array(dist_row_names)\n",
    "rep_dist_df = pd.DataFrame(np.vstack((rep_names, rep_dists)).T, columns=['metric', 'dist'])\n",
    "rep_dist_df.loc[:, 'metric'] = rep_dist_df.loc[:, 'metric'].astype('category')\n",
    "rep_dist_df.loc[:, 'dist'] = rep_dist_df.loc[:, 'dist'].astype('float')\n",
    "\n",
    "plt.clf(); plt.close();\n",
    "figrep, axrep = plt.subplots(nrows=1, ncols=1, figsize=(6,6), dpi=120)\n",
    "ax = sns.violinplot(y=\"dist\", x=\"metric\", data=rep_dist_df, ax=axrep)\n",
    "ax.yaxis.label.set_size(0)\n",
    "ax.xaxis.label.set_size(0)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "figrep.savefig(\"../otu_data/trim_stats/replicate_distances.png\", dpi=120)\n",
    "plt.show()\n",
    "\n",
    "abund_md_df_derep = abund_md_df[(abund_md_df.replicate == 1) & (abund_md_df.DepthName != 'LAB')]\n",
    "print(abund_md_df_derep.shape, abund_md_df.shape, rep_dist_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2: Hierarchical Clustering of Principal Components w/ Metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabaz_score, silhouette_samples\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# There are three ways of doing this: using rclr+euclidean, bray-curtis, or unifrac\n",
    "\n",
    "#score_types = ['CalinskiHarabaz', \"SilhouetteScore\"]\n",
    "#suffix_types = [ \"_clr\", \"_bc\", \"_wu\"]\n",
    "\n",
    "score_types = [\"SilhouetteScore\"]\n",
    "suffix_types = [ \"_wu\"]\n",
    "linkage_types = [\"complete\", \"average\", 'single']\n",
    "\n",
    "cluster_params = [(i, j, k, j+i+\"_\"+k) for i in suffix_types for j in score_types for k in linkage_types]\n",
    "print(list(zip(*cluster_params))[3][0])\n",
    "clust_num_range = list(range(2,30))\n",
    "clust_scores = pd.DataFrame(index=clust_num_range, columns=list(zip(*cluster_params))[3]).astype(float)\n",
    "met_dict = {'CalinskiHarabaz':calinski_harabaz_score, \"SilhouetteScore\":silhouette_samples}\n",
    "\n",
    "dist_dict = {}\n",
    "for st, sct, lkt, colname_ in cluster_params:\n",
    "    print(st, sct, lkt, colname_)\n",
    "    suff_cols = [i for i in abund_md_df_derep.columns if st in i and i[:(-1*len(st))] in abund_md_df_derep.index]\n",
    "    assert len(suff_cols) == len(abund_md_df_derep.index)\n",
    "    normval = 1\n",
    "    for i, j in zip(abund_md_df_derep.index, suff_cols):\n",
    "        assert i == j[:(-1*len(st))]\n",
    "    precomputed_dists = abund_md_df_derep.loc[:, suff_cols]\n",
    "    dist_dict[st] = precomputed_dists.copy()\n",
    "    if sct == 'SilhouetteScore':\n",
    "        normval = 1\n",
    "    for n_clusters_ in clust_num_range:\n",
    "        cluster_mod = AgglomerativeClustering(n_clusters=n_clusters_, affinity='precomputed', linkage=lkt) \n",
    "        cluster_labels = cluster_mod.fit_predict(precomputed_dists.values)\n",
    "        with_misclassified = met_dict[sct](precomputed_dists.values, cluster_labels)\n",
    "        clust_scores.loc[n_clusters_, colname_] = with_misclassified[with_misclassified > 0].mean()\n",
    "\n",
    "plt.close(); plt.clf();\n",
    "fig_cscore, ax_cscore = plt.subplots(nrows=1, ncols=1, figsize=(6, 6), dpi=120)\n",
    "clust_scores.SilhouetteScore_wu_average.plot(ax=ax_cscore)\n",
    "plt.grid()\n",
    "fig_cscore.savefig(\"../otu_data/pca_plots/SilhouetteScore_WeightedUnifrac.png\")\n",
    "plt.show()\n",
    "plt.close(); plt.clf();\n",
    "for sc in clust_scores.columns:\n",
    "    print(sc, clust_scores[sc].idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# clean clusters\n",
    "for n_clusters in [5, 9, 12]:\n",
    "    best_mod = AgglomerativeClustering(n_clusters=n_clusters, affinity='precomputed', linkage='average') \n",
    "    wu_dists = dist_dict['_wu']\n",
    "    cluster_labels = best_mod.fit_predict(wu_dists.values)\n",
    "    sample_silhouette_values = silhouette_samples(wu_dists.values, cluster_labels)\n",
    "    positive_idxs = wu_dists.index[sample_silhouette_values > -1]\n",
    "    positive_silhouettes = sample_silhouette_values[sample_silhouette_values > -1]\n",
    "    print(positive_silhouettes.mean())\n",
    "    positive_labels = cluster_labels[sample_silhouette_values > -1]\n",
    "    label_counts = np.unique(positive_labels, return_counts=1)\n",
    "\n",
    "    good_clusts = label_counts[0][label_counts[1] > 6]\n",
    "    print(\"all clusters\", set(cluster_labels))\n",
    "    print(\"good clusters\", good_clusts)\n",
    "    print(\"total clustered\", label_counts[1][label_counts[1] > 0].sum())\n",
    "\n",
    "    fig, ax1 = plt.subplots(nrows=1, ncols=1, dpi=120, figsize=(6,6))\n",
    "    ax1.set_xlim([-.45, .8])\n",
    "    ax1.set_ylim([0, len(wu_dists.values) + ((1+len(good_clusts))* 10) ])\n",
    "    y_lower = 10\n",
    "\n",
    "    clean_clusts = {}\n",
    "    for i in good_clusts:\n",
    "        print(i)\n",
    "        ith_cluster_silhouette_values = positive_silhouettes[positive_labels == i]\n",
    "        clean_clusts[i] = positive_idxs[positive_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        color = cm.nipy_spectral((float(i)*2) / (2*n_clusters))\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.35, y_lower + 0.5 * size_cluster_i, \"Cluster {} : ({})\".format(i, size_cluster_i), fontsize=14)\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for cleaned clusters.\", fontsize=14)\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\", fontsize=14)\n",
    "    ax1.set_ylabel(\"Cluster label\", fontsize=14)\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(\"../otu_data/pca_plots/sample_silhouettes_for_{}_clusters.png\".format(n_clusters))\n",
    "    plt.clf(); plt.close();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "import matplotlib\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import scipy.spatial.distance as ssd\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "clean_labs = []\n",
    "for c_i, (ccn, ccv) in enumerate(clean_clusts.items()):\n",
    "    print(c_i, \"Clust {} has {} members: {}\".format(ccn, len(ccv), ccv[0]))\n",
    "    clean_labs += list(ccv)\n",
    "    \n",
    "ordered_index = [i for i in wu_dists.index if i in clean_labs]\n",
    "ordered_columns = [i for i in wu_dists.columns if i[:-3] in clean_labs]\n",
    "cleaned_df = wu_dists.loc[ordered_index, ordered_columns]\n",
    "sqmat = ssd.squareform(cleaned_df.values)\n",
    "toplim, botlim = sqmat.max(), sqmat[sqmat > 0.0].min()\n",
    "\n",
    "# fix abundance and metadata tables\n",
    "abund_md_df_clust = abund_md_df.loc[ordered_index, :]\n",
    "\n",
    "opt_clusts = len(clean_clusts.values())\n",
    "plt.clf()\n",
    "# Override the default linewidth.\n",
    "matplotlib.rcParams['lines.linewidth'] = 0.5\n",
    "Z = hierarchy.linkage(sqmat, 'average', optimal_ordering=True)\n",
    "fig_ = plt.figure(figsize=(7, 6), dpi=250)\n",
    "gs = gridspec.GridSpec(5, 4, figure=fig_, wspace=0.01, hspace=0.01, \n",
    "                       height_ratios=[75,5,5,5,5], width_ratios=[18,.4,1.3,1.3])\n",
    "ax1 = plt.subplot(gs[0,0])\n",
    "\n",
    "\n",
    "for c_height in np.arange(botlim*0.1, toplim*1.1, botlim*.01):\n",
    "    cuttree = hierarchy.cut_tree(Z, height=[c_height])\n",
    "    if len(set(list(cuttree[:, 0]))) <= opt_clusts:\n",
    "        break\n",
    "\n",
    "#c_height = 0.55\n",
    "\n",
    "print(\"optimal minimum cophenetic distance is {}\".format(c_height))\n",
    "hierarchy.set_link_color_palette(['tab:blue', 'tab:orange', 'tab:green', 'tab:red', \n",
    "                                  'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', \n",
    "                                  'tab:olive', 'tab:cyan'])\n",
    "\n",
    "res = hierarchy.dendrogram(Z, color_threshold=c_height*1.01, get_leaves=True, \n",
    "                           ax=ax1, labels=cleaned_df.index, no_labels=True, \n",
    "                           above_threshold_color='k')\n",
    "ax1.axhline(y=c_height*1.01)\n",
    "ax1.axis('off')\n",
    "matplotlib.rcParams['lines.linewidth'] = 1.5\n",
    "\n",
    "# get axis order\n",
    "new_idx = cleaned_df.index[res['leaves']]\n",
    "# metadata columns to plot\n",
    "sel_md_cols = ['Month_Year', 'StationName', #'Pycnocline', \n",
    "               'Salinity_Group', 'sequencing ID', 'CollectionAgency']\n",
    "# resort metadata\n",
    "clustered_metadata = abund_md_df_derep.loc[new_idx, sel_md_cols].copy()\n",
    "# encode colors and reverse\n",
    "color_codings = [{1:'06 15', 2:'07 15', 3:'08 15', 7:'06 16', 8:'07 16', 9:'08 16', \n",
    "                  13:'04 17', 14:'05 17', 15:'06 17', 16:'07 17', 17:'08 17', 20:'09 17'},\n",
    "                 {4:'CB33C', 1:'CB22', 5:'CB41C', 7:'CB43C', 8:'CB44', 9:'CB51', 10:'CB52', \n",
    "                  11:'CB53', 12:'CB54', 17:'CB71', 14:'CB62', 15:'CB63', 18:'CB72', 16:'CB64', \n",
    "                  20:'CB74', 2:'CB31', 3:'CB32', 6:'CB42C', 13:'CB61', 19:'CB73'},\n",
    "                 #{1:'Above', 20:'Below', 10:np.nan}, \n",
    "                 {10:'Mesohaline', 1:'Oligohaline', 20:'Polyhaline'},\n",
    "                 {1:'sprehei1_123382', 4:'Miseq_data_SarahPreheim_Sept2016', 7:'esakows1_132789', \n",
    "                  10:'Keith_Maeve1_138650', 13:'sprehei1_149186', 17:'esakows1_152133_plate_1', \n",
    "                  20:'esakows1_152133_plate_2'}, {1:\"Preheim\", 10:\"ODU\", 20:\"DNR\"}]\n",
    "\n",
    "rev_col_codes = []\n",
    "for x in color_codings:\n",
    "    rev_col_codes.append({j:i for i, j in x.items()})\n",
    "\n",
    "# map encodings\n",
    "for sdc, cmapping in zip(sel_md_cols, rev_col_codes):\n",
    "    clustered_metadata[sdc] = clustered_metadata[sdc].map(cmapping)\n",
    "\n",
    "# select palette with max # of categories\n",
    "#all_col_pal = sns.color_palette(\"cubehelix_r\", clustered_metadata.max().max())\n",
    "all_col_pal = sns.cubehelix_palette(n_colors=clustered_metadata.max().max(), start=.3, rot=1.5, gamma=1.0,\n",
    "                                    hue=0.7, light=0.95, dark=0.25, reverse=0)\n",
    "cmap_ = LinearSegmentedColormap.from_list('Custom', tuple(all_col_pal), len(tuple(all_col_pal)))\n",
    "\n",
    "mdata_nrow = clustered_metadata.shape[0]\n",
    "# plot first row and bar\n",
    "ax2 = plt.subplot(gs[1,0])\n",
    "ax2l = plt.subplot(gs[1,1:])\n",
    "ax2_vals = clustered_metadata['Month_Year'].values.reshape(mdata_nrow,1).T\n",
    "sns.heatmap(ax2_vals, linewidths=0.0, cmap=cmap_, cbar=False, xticklabels=False, yticklabels=False, ax=ax2)\n",
    "ax2l.text(0.,0.4, \"Sampling Date\", fontdict={'fontsize':8})\n",
    "ax2.axis('off'); ax2l.axis('off');\n",
    "\n",
    "ax4 = plt.subplot(gs[2,0])\n",
    "ax3 = plt.subplot(gs[0,1])\n",
    "ax4_vals = clustered_metadata['StationName'].values.reshape(mdata_nrow,1).T\n",
    "stat_ax = sns.heatmap(ax4_vals, linewidths=0.0, cmap=cmap_, cbar=True, cbar_ax=ax3, xticklabels=False, yticklabels=False, ax=ax4)\n",
    "cb = stat_ax.collections[0].colorbar\n",
    "cb.ax.tick_params(labelsize=6);\n",
    "cb.set_ticks([1, 20]); cb.set_ticklabels(['Min', 'Max']); \n",
    "ax4l = plt.subplot(gs[2,1:])\n",
    "ax4l.text(0.,0.4, \"Station\", fontdict={'fontsize':8})\n",
    "ax4.axis('off'); ax4l.axis('off');\n",
    "\n",
    "ax6 = plt.subplot(gs[3,0])\n",
    "ax6l = plt.subplot(gs[3,1:])\n",
    "ax6_vals = clustered_metadata['Salinity_Group'].values.reshape(mdata_nrow,1).T\n",
    "sns.heatmap(ax6_vals, linewidths=0.0, cmap=cmap_, cbar=False, xticklabels=False, yticklabels=False, ax=ax6)\n",
    "ax6l.text(0.,0.4, \"Salinity Region\", fontdict={'fontsize':8})\n",
    "ax6.axis('off'); ax6l.axis('off');\n",
    "\n",
    "#ax7 = plt.subplot(gs[4,0])\n",
    "#ax7l = plt.subplot(gs[4,1:])\n",
    "#ax7_vals = clustered_metadata[\"Pycnocline\"].values.reshape(mdata_nrow,1).T\n",
    "#sns.heatmap(ax7_vals, linewidths=0.0, cmap=cmap_, cbar=False, xticklabels=False, yticklabels=False, ax=ax7)\n",
    "#ax7l.text(0.,0.4, \"Pycnocline\", fontdict={'fontsize':6})\n",
    "#ax7.axis('off'); ax7l.axis('off');\n",
    "\n",
    "ax8 = plt.subplot(gs[4,0])\n",
    "ax8l = plt.subplot(gs[4,1:])\n",
    "ax8_vals = clustered_metadata[\"CollectionAgency\"].values.reshape(mdata_nrow,1).T\n",
    "sns.heatmap(ax8_vals, linewidths=0.0, cmap=cmap_, cbar=False, xticklabels=False, yticklabels=False, ax=ax8)\n",
    "ax8l.text(0.,0.4, \"Agency\", fontdict={'fontsize':8})\n",
    "ax8.axis('off'); ax8l.axis('off');\n",
    "\n",
    "fig_.savefig('../otu_data/pca_plots/pca_of_wu_avlinkage_clusters_clean.png', dpi=250)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets make PCA plots colored by metadata columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deicode.optspace import OptSpace\n",
    "\n",
    "rename_cols = {i - 1: 'PC' + str(i) for i in range(1, 3)}\n",
    "\n",
    "print(\"Decomposing {} dist table {}\".format('Weighted Unifrac', cleaned_df.shape))\n",
    "opt_i = OptSpace(rank=2).fit(cleaned_df.values)\n",
    "sl_i = pd.DataFrame(opt_i.sample_weights, index=cleaned_df.index)\n",
    "sample_loadings = sl_i.rename(columns=rename_cols)\n",
    "\n",
    "rev_cc_mapping = {k:\"Clust\"+str(i) for i,j in clean_clusts.items() for k in j}\n",
    "\n",
    "clustcolors = [cm.nipy_spectral((float(i)*2) / 24) for i in range(12)]\n",
    "keithcm = LinearSegmentedColormap.from_list('keithspec', clustcolors, N=len(clustcolors))\n",
    "\n",
    "mdf = pd.DataFrame(index=cleaned_df.index, columns=['ClustAssgns'])\n",
    "mdf['ClustAssgns'] = cleaned_df.reset_index()['Samples'].map(rev_cc_mapping).values\n",
    "\n",
    "from skbio import DistanceMatrix\n",
    "from skbio.stats.ordination import pcoa\n",
    "plt.clf(); plt.close();\n",
    "dm = DistanceMatrix(cleaned_df.values, cleaned_df.index)\n",
    "pcoa_results = pcoa(dm)\n",
    "fig = pcoa_results.plot(df=mdf, column='ClustAssgns',\n",
    "                        title='', cmap=keithcm, s=25)\n",
    "\n",
    "fig.set_size_inches(w=8, h=6, forward=True)\n",
    "fig.suptitle(\"PCoA of Weighted Unifrac\", x=0.34, y=0.95, fontsize=14)\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('PC1', fontsize=12)\n",
    "ax.set_ylabel('PC2', fontsize=12)\n",
    "ax.set_zlabel('PC3', fontsize=12)\n",
    "fig.set_dpi(120)\n",
    "fig.savefig(\"../otu_data/pca_plots/3d_pca_wu_coloredbyclust.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "clean_join = lambda x: tuple([x[0].strip(), x[1].strip()])\n",
    "melted_dists['pair'] = melted_dists.loc[:, ['OTUONE', 'OTUTWO']].apply(clean_join, axis=1)\n",
    "\"\"\"\n",
    "# define interval \n",
    "short_part_upper = np.arange(0.01,0.06, 0.01)\n",
    "short_part_lower = short_part_upper - 0.01\n",
    "longer_part_upper = np.arange(0.05,0.45, 0.05)\n",
    "longer_part_lower = longer_part_upper - 0.05\n",
    "upper_part = np.hstack((short_part_upper, longer_part_upper))\n",
    "lower_part = np.hstack((short_part_lower, longer_part_lower))\n",
    "\n",
    "for up_lim, low_lim in zip(upper_part, lower_part):\n",
    "    top_chop = melted_dists.Distance < up_lim\n",
    "    low_rows = melted_dists.Distance >= low_lim\n",
    "    dist_bin = melted_dists[top_chop & low_rows]\n",
    "    print(\"OTUS separated by between {} and {} units in phylogenetic space: {}\".format(low_lim, up_lim, \n",
    "                                                                                       dist_bin.shape))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-talk')\n",
    "\n",
    "mdata_clean = meta_df_x.loc[]\n",
    "print(select_metadata)\n",
    "select_mdata = select_metadata + ['Latitude', 'Longitude', 'RawCount_b', 'TrimCount_b', 'Cluster']\n",
    "\n",
    "metas_ls = {'encoded':{}, 'raw':{}, 'encoding':{}, 'rev_coding':{}}\n",
    "for sm in select_mdata:\n",
    "    nct = meta_df_x[sm].isnull().sum()\n",
    "    metas_ls['raw'][sm] = meta_df_x[sm].tolist()\n",
    "    metas_ls['encoding'][sm] = {raw:code for code, raw in enumerate(sorted(set(metas_ls['raw'][sm])))}\n",
    "    metas_ls['encoded'][sm] = [metas_ls['encoding'][sm][r] for r in metas_ls['raw'][sm]]\n",
    "    metas_ls['rev_coding'][sm] = {code:raw for raw, code in metas_ls['encoding'][sm].items()}\n",
    "    uct = len(metas_ls['encoding'][sm])\n",
    "\n",
    "title_i = 'WeightedUnifracDists'\n",
    "\n",
    "plt.clf()\n",
    "fig, ax_i = plt.subplots(nrows=1, ncols=1, figsize=(12,12), dpi=250)\n",
    "ax_i.set_title(\"{}, colored by sequencing run\".format(title_i))\n",
    "ticks_, labels_ = zip(*metas_ls['rev_coding']['sequencing_ID'].items())\n",
    "cmap_i = plt.cm.get_cmap('Spectral', len(labels_))\n",
    "im = ax_i.scatter(sample_loadings.iloc[:, 0], \n",
    "                  sample_loadings.iloc[:, 1], \n",
    "                  c=metas_ls['encoded']['sequencing_ID'], \n",
    "                  edgecolor='k', alpha=0.8, cmap=cmap_i)\n",
    "cbar = fig.colorbar(im, ticks=ticks_)\n",
    "cbar.ax.set_yticklabels(labels_)     \n",
    "ax_i.set_xlabel(sample_loadings.columns[0])\n",
    "ax_i.set_ylabel(sample_loadings.columns[1])\n",
    "ax_i.set_facecolor('0.6')\n",
    "ax_i.set_axisbelow(True)\n",
    "ax_i.minorticks_on()\n",
    "ax_i.grid(which='major', linestyle='-', linewidth='0.5', color='gray')\n",
    "ax_i.grid(which='minor', linestyle='-', linewidth='0.25', color='black')\n",
    "#    texts = []\n",
    "#    from adjustText import adjust_text\n",
    "#    for ctrl_ in control_libs:\n",
    "#        x = sample_loadings[title_i].loc[ctrl_, 'PC1']\n",
    "#        y = sample_loadings[title_i].loc[ctrl_, 'PC2']\n",
    "#        s = meta_data_df2.loc[ctrl_, 'Short sample name'] + \"_\" + sid_map[meta_data_df2.loc[ctrl_, 'sequencing ID']]\n",
    "#        texts.append(plt.text(x, y, s))       \n",
    "#    adjust_text(texts, arrowprops=dict(arrowstyle=\"->\", color='r', lw=0.5))\n",
    "fig.subplots_adjust(right=0.8)\n",
    "figname = \"PCA_{}_abthresh_seq_run_colors.png\".format(title_i)\n",
    "figpath = os.path.join(\"../otu_data/pca_plots\", figname)\n",
    "print(\"Saving {}\".format(figname))\n",
    "plt.savefig(figpath)\n",
    "plt.clf()\n",
    "plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "# calculate cluster sums \n",
    "\n",
    "temp_amdf = abund_md_df_clust.reset_index()\n",
    "temp_amdf['Cluster Label'] = temp_amdf['Samples'].map(rev_cc_mapping)\n",
    "ab_md_df_dr_cl = temp_amdf.set_index('Samples')\n",
    "cols_to_group = list(rare_abund.columns)+['Cluster Label']\n",
    "ab_df_dr_cl = ab_md_df_dr_cl.loc[:, cols_to_group]\n",
    "cluster_sums = ab_df_dr_cl.groupby('Cluster Label').agg(np.sum).sort_index()\n",
    "print(cluster_sums.sum(1))\n",
    "\n",
    "def taxa_breakdown(abunds_, taxas_, level_, weighted=True, flatten_val=0.0):\n",
    "    # 'Kingdom', 'Phylum', 'Class', 'Order', 'Family', 'Genus', 'Species'\n",
    "    # remove non-existant features\n",
    "    flip_abunds = abunds_.loc[:, abunds_.sum(0) > 0].T\n",
    "    # create presence or absence table if need be\n",
    "    if not weighted:\n",
    "        flip_abunds = (flip_abunds > 0).astype(int)\n",
    "    # add level column\n",
    "    otu_fetch = lambda x: taxas_.loc[x, level_]\n",
    "    flip_abunds['otu_name'] = flip_abunds.index\n",
    "    flip_abunds['taxa_name'] = flip_abunds['otu_name'].apply(otu_fetch)\n",
    "    flip_abunds.drop('otu_name', axis=1, inplace=True)\n",
    "    ttable_raw = flip_abunds.groupby('taxa_name').agg(np.sum)\n",
    "    ttable = ttable_raw.div(ttable_raw.sum(0))\n",
    "    if flatten_val:\n",
    "        flat_ttv = ttable.values\n",
    "        flat_ttv[flat_ttv < flatten_val] = 0.0\n",
    "        ttable = pd.DataFrame(flat_ttv, index=ttable.index, columns=ttable.columns)\n",
    "    return ttable.T\n",
    "\n",
    "\n",
    "abunds_1 = cluster_sums.copy()\n",
    "taxas_1 = taxa_df.copy().astype(str)\n",
    "for level_1 in ['Class', 'Order']:\n",
    "    ttable_1 = taxa_breakdown(abunds_1, taxas_1, level_1, weighted=True, flatten_val=0.01)\n",
    "    print(ttable_1.shape)\n",
    "    ttable_1 = ttable_1.loc[:, ttable_1.columns[ttable_1.sum() > 0]]\n",
    "    print(\"The collapsed taxa table is {}\".format(ttable_1.shape))\n",
    "\n",
    "    col_order = ttable_1.max().sort_values(ascending=False).index\n",
    "    ttable_1 = ttable_1.loc[:, col_order]\n",
    "\n",
    "    fignamet = \"TaxaClusters_{}.png\".format(level_1)\n",
    "    figpatht = os.path.join(\"../otu_data/pca_plots\", fignamet)\n",
    "\n",
    "    plt.clf(); plt.close();\n",
    "    fig_width = 8\n",
    "    fig_t = plt.figure(figsize=(fig_width,10), dpi=140)\n",
    "    gs = gridspec.GridSpec(2, 1, figure=fig_t, height_ratios=[7,4], hspace=.2,\n",
    "                           bottom=0.075, top=0.925, right=0.925, left=0.075)\n",
    "    ax_arr = [plt.subplot(gs[0,0])]\n",
    "\n",
    "    possible_colors = [j for i, j in sns.xkcd_rgb.items() if not 'white' in i]\n",
    "    np.random.seed(2)\n",
    "    colors_needed = np.random.choice(possible_colors, size=ttable_1.columns.shape)\n",
    "    print(\"{} colors grabbed\".format(len(colors_needed)))\n",
    "\n",
    "    # loop over each table to plot\n",
    "    axis_titles = ['Aggregate Cluster Taxonomy']\n",
    "    for ax_ix, table_x in enumerate([ttable_1]):\n",
    "        # pick an axis\n",
    "        ax_i = ax_arr[ax_ix]\n",
    "        ax_i.set_title(axis_titles[ax_ix])\n",
    "        # set the width of each bar to the number of samples\n",
    "        adjusted_width = (fig_width / table_x.shape[0])*(.8)\n",
    "        # set the left bottom anchor of each bar\n",
    "        bar_locs = np.arange(table_x.shape[0])*(fig_width / table_x.shape[0])\n",
    "        # set the bar labels \n",
    "        bar_names = table_x.index\n",
    "        # loop over each taxon name\n",
    "        for bar_n, bar_col in enumerate(table_x.columns):\n",
    "            # subset those fractions across samples\n",
    "            bar_x = table_x.loc[:, bar_col]\n",
    "            # set the y-axis location for each bar\n",
    "            if bar_n == 0:\n",
    "                running_base = bar_x*0.0\n",
    "            # Create an individual bar\n",
    "            ax_i.bar(bar_locs, bar_x.values, bottom=running_base.values, \n",
    "                     color=colors_needed[bar_n], edgecolor='white', \n",
    "                     width=adjusted_width, tick_label=bar_names)\n",
    "            for tick in ax_i.get_xticklabels():\n",
    "                tick.set_rotation(45)\n",
    "            # increment the bottoms\n",
    "            running_base = running_base + bar_x\n",
    "\n",
    "    ax2 = plt.subplot(gs[1,0])\n",
    "    patches = [mpatches.Patch(color=color, label=label) for label, color in zip(list(ttable_1.columns), colors_needed)]\n",
    "    ax2.legend(patches, list(ttable_1.columns), loc='best', bbox_to_anchor=(0., 0., 1., 1.),\n",
    "               mode='expand', fontsize='x-small', ncol=3)\n",
    "\n",
    "    ax2.axis('off')\n",
    "    # Show graphic\n",
    "    plt.show()\n",
    "    print(\"Saving {}\".format(fignamet))\n",
    "    fig_t.savefig(figpatht, dpi=140)\n",
    "    plt.show();\n",
    "    plt.clf(); plt.close();\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is where we write out the rarefied matrix and calculate (if not done) and write out alpha diversity stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abund_md_df_clust['Cluster Label'] = ab_md_df_dr_cl.loc[abund_md_df_clust.index, 'Cluster Label'].copy()\n",
    "\n",
    "cols_to_check = ['StationName', 'Month_Year', 'DepthName', 'Month', \n",
    "                 'Year', 'Salinity_Group', 'sequencing_ID']\n",
    "for clust in abund_md_df_clust['Cluster Label'].unique():\n",
    "    subdf = abund_md_df_clust[abund_md_df_clust['Cluster Label'] == clust]\n",
    "    subdf_alpha = subdf.loc[:, ['faith_pd', 'observed_otus', 'enspie']]\n",
    "    pct_fxn = lambda x: np.percentile(x, [0, 2.5, 50, 97.5, 100])\n",
    "    alpha_stats = subdf_alpha.apply(pct_fxn)\n",
    "    num_samps = subdf.shape[0]\n",
    "    print(clust, num_samps)\n",
    "    print(alpha_stats)\n",
    "    for col_s in cols_to_check:\n",
    "        n_x, x_x = np.unique(subdf[col_s].values, return_counts=1)\n",
    "        print(col_s, len(n_x))\n",
    "        for n, x in zip(n_x, x_x):\n",
    "            print(\"\\t{}: {:.2%}\".format(n, x/num_samps))        \n",
    "        input()\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPARCC Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparcc_dir = \"../otu_data/sparcc_data\"\n",
    "sparcc_file = os.path.join(sparcc_dir, \"filtered_otu_table.txt\")\n",
    "if os.path.exists(sparcc_file):\n",
    "    just_abunds = pd.read_csv(sparcc_file, sep=\"\\t\", index_col=0).T\n",
    "else:\n",
    "    just_abunds = abund_md_df_derep.loc[:, abund_df_og_s1.columns].T\n",
    "    just_abunds.to_csv(sparcc_file, sep=\"\\t\", index_label='OTU_id')\n",
    "\n",
    "print(just_abunds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPARCC In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "corrs_file = \"/Volumes/KeithSSD/CB_V4/otu_data/sparcc_data/sparcc_corr.out\"\n",
    "df = pd.read_csv(corrs_file, sep=\"\\t\", index_col=0)\n",
    "pval_file = \"/Volumes/KeithSSD/CB_V4/otu_data/sparcc_data/test_pvals.two_sided.txt\"\n",
    "p_df = pd.read_csv(pval_file, sep=\"\\t\", index_col=0)\n",
    "\n",
    "assert df.index.equals(p_df.index)\n",
    "assert df.columns.equals(p_df.columns)\n",
    "\n",
    "def melt_upper_triangle(df_, val_str):\n",
    "    dfnan = df_.where(np.triu(np.ones(df_.shape)).astype(np.bool))\n",
    "    melted_df = dfnan.stack().reset_index()\n",
    "    melted_df.columns = ['OTU_1','OTU_2', val_str]\n",
    "    melted_df2 = melted_df[melted_df['OTU_1'] != melted_df['OTU_2']]\n",
    "    return melted_df2.set_index(['OTU_1', 'OTU_2'])\n",
    "\n",
    "mpdf = melt_upper_triangle(p_df, 'p-value')\n",
    "mdf = melt_upper_triangle(df, 'correlation')\n",
    "\n",
    "fulldf = mdf.join(mpdf)\n",
    "\n",
    "# pull total abundances\n",
    "# pull taxonomy (order?)\n",
    "\n",
    "reject, pvals_corrected = multipletests(fulldf['p-value'].values, alpha=0.05, method='fdr_bh')[:2]\n",
    "\n",
    "thresholded = fulldf.loc[fulldf.index[reject], ['correlation']].reset_index()\n",
    "\"\"\"\n",
    "corr_cutoff = abs(thresholded.correlation) > 0.3\n",
    "thresholded_cutoff = thresholded[corr_cutoff]\n",
    "print(thresholded_cutoff.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# conver to relative abundance \n",
    "ja_ra = just_abunds.div(just_abunds.sum(1), axis=0)\n",
    "\n",
    "thresh_plus_scores = thresholded_cutoff.copy()\n",
    "# pool by cluster i.e. sum rows and divide by number of them\n",
    "clust_ra = pd.DataFrame(index=clean_clusts.keys(), columns=ja_ra.columns)\n",
    "for clust, membs in clean_clusts.items():\n",
    "    n_mems = len(membs)\n",
    "    clust_row = ja_ra.loc[membs, :].sum() / n_mems\n",
    "    clust_ra.loc[clust, :] = clust_row\n",
    "    pool_clust = lambda x: clust_ra.loc[clust, [x[0], x[1]]].sum()\n",
    "    colname = \"clust_{}_score\".format(clust)\n",
    "    thresh_plus_scores[colname] = thresh_plus_scores.loc[:, ['OTU_1', 'OTU_2']].apply(pool_clust, axis=1)\n",
    "    print(colname, 'done')\n",
    "    \n",
    "order_otu_one = thresh_plus_scores.OTU_1.apply(lambda x: taxa_df.loc[x, 'Phylum'])\n",
    "order_otu_two = thresh_plus_scores.OTU_2.apply(lambda x: taxa_df.loc[x, 'Phylum'])\n",
    "order_otu_two.name = \"OTU_2_Taxonomy\"\n",
    "order_otu_one.name = \"OTU_1_Taxonomy\"\n",
    "thresh_cut_taxa = thresh_plus_scores.join(order_otu_one).join(order_otu_two)\n",
    "thresh_cut_taxa.to_csv(\"/Volumes/KeithSSD/CB_V4/otu_data/sparcc_data/corr_net.txt\", sep=\"\\t\", index=False)\n",
    "thresh_cut_taxa.head()\n",
    "\"\"\"\n",
    "ca_colnorm = clust_ra.div(clust_ra.sum(0), axis=1)\n",
    "\n",
    "print(\"Top {} OTU Intersections\")\n",
    "for clust1 in clean_clusts.keys():\n",
    "    c1_row = ca_colnorm.loc[clust1, :].sort_values(ascending=False)\n",
    "    c1_anchors = set(c1_row[c1_row == 1].index)\n",
    "    in_first_col = thresholded_cutoff.OTU_1.isin(c1_anchors)\n",
    "    in_second_col = thresholded_cutoff.OTU_2.isin(c1_anchors)\n",
    "    retained_rows = thresholded_cutoff[in_first_col | in_second_col]\n",
    "    retained_pct = retained_rows.shape[0] / thresh_cut_taxa.shape[0]\n",
    "    print(\"{} has {} anchors which retain {}/{:.2%} of edges in graph\".format(clust1, len(c1_anchors),\n",
    "                                                                              retained_rows.shape[0], retained_pct))\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This reformats the guppy distance matrix into something usable and symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_file = \"../otu_data/tree_data/full_tree/query_cmsearched.hug_tol.clean.align.dist.tab\"\n",
    "\n",
    "triang_arr = [[0]]\n",
    "rec_n = 1\n",
    "with open(dist_file, \"r\") as dih:\n",
    "    for ix, l in enumerate(dih):\n",
    "        rec_n += 1\n",
    "        triang_arr.append(l.replace(\"S\", \"\").replace(\"P\", \"\").split()+[0.])\n",
    "        \n",
    "\n",
    "full_arr = np.array([x+[0.0]*(rec_n-len(x)) for x in triang_arr], dtype=float)\n",
    "dist_df = pd.DataFrame(full_arr + full_arr.T)\n",
    "dist_df.index = list(dist_df.index)\n",
    "dist_df.columns = list(dist_df.columns)\n",
    "print(dist_df.shape)\n",
    "print(dist_df.index[:5])\n",
    "print(dist_df.columns[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will read in pplacer file and collapse the abundance table according to edge placements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read in the pplacer placements data\n",
    "pplacements = '../otu_data/tree_data/full_tree/query_cmsearched.hug_tol.clean.align.csv'\n",
    "pp_df = pd.read_csv(pplacements, index_col=1)\n",
    "\n",
    "# calculate likelihood statistics per edge\n",
    "binned_lwr = pp_df.loc[:, ['edge_num', 'like_weight_ratio']].groupby('edge_num').agg(['mean', 'std'])\n",
    "\n",
    "# remove really shitty edges (high std, low likelihood)\n",
    "rough_edges = set()\n",
    "rough_edges.update(binned_lwr[binned_lwr.loc[:, binned_lwr.columns[1]] > 0.3].index)\n",
    "rough_edges.update(binned_lwr[binned_lwr.loc[:, binned_lwr.columns[0]] < 0.2].index)\n",
    "\n",
    "# create a map and dataframe to for pooling placements by edge\n",
    "idx_bools = {idx:pp_df[pp_df.edge_num.isin([idx])].index for idx in sorted(pp_df.edge_num.unique())}\n",
    "pooled_pp_df = pd.DataFrame(index=abund_df_jm.index, columns=sorted(pp_df.edge_num.unique())).fillna(0.)\n",
    "for idx_, otus_ in idx_bools.items():\n",
    "    pooled_pp_df.loc[:, idx_] = abund_df_jm.loc[:, otus_].sum(1)\n",
    "\n",
    "# ensure the sumes of the original and pooled tables are identical\n",
    "assert pooled_pp_df.sum().sum() - abund_df_jm.sum().sum() == 0\n",
    "print(\"After clustering {} features reduced to {}\".format(abund_df_jm.shape[1], pooled_pp_df.shape[1]))\n",
    "\n",
    "# remove controls and low abundance edges and low yield samples\n",
    "pooled_pp_ns = decrease_sparsity(pooled_pp_df.copy(), control_libs, addl_keys=['Zymo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check how tight our pools are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(List): \n",
    "    return max(set(List), key = List.count)\n",
    "\n",
    "# filter out edges in rough_edges and those without relatively fine scale taxonomic classification \n",
    "good_edges = {}\n",
    "for edge in pooled_pp_ns.columns:\n",
    "    members = idx_bools[edge]\n",
    "    if not edge in rough_edges:\n",
    "        taxa_rows = taxa_df.loc[members, :]\n",
    "        if taxa_rows.shape[0] > 2:\n",
    "            classified_ = taxa_rows.isnull().sum() / taxa_rows.shape[0] < 0.5\n",
    "            if classified_[classified_].shape[0] > 0:\n",
    "                lowest_level = classified_[classified_].index[-1]\n",
    "                if not lowest_level in ['Kingdom', 'Phylum',]:\n",
    "                    good_edges[edge] = [most_frequent(taxa_rows.loc[:, i].tolist()) for i in taxa_df.columns]\n",
    "        else:\n",
    "            good_edges[edge] = [most_frequent(taxa_rows.loc[:, i].tolist()) for i in taxa_df.columns]\n",
    "\n",
    "# this is some code to check intra- and inter- distances between taxonomic classes\n",
    "edge_taxa = pd.DataFrame(good_edges, index=taxa_df.columns).T\n",
    "for level in taxa_df.columns:\n",
    "    class_names, class_counts = np.unique(edge_taxa[level].dropna().values, return_counts=1)\n",
    "    common_classes = list(class_names[class_counts > 4])\n",
    "    max_combos = int((len(common_classes)*(len(common_classes)-1))/2)\n",
    "    print(\"{} has {} common classes (comparisons = {})\".format(level, len(common_classes), max_combos))\n",
    "    check_counter = 0 \n",
    "    for mc in range(max_combos):\n",
    "        np.random.shuffle(common_classes)\n",
    "        c_1 = common_classes[0]\n",
    "        c_2 = common_classes[1]\n",
    "        print(\"\\tChecking {} and {}\".format(c_1, c_2))\n",
    "        check_counter += 1\n",
    "        for iter_cnt in range(10):\n",
    "            np.random.seed(iter_cnt*50)\n",
    "            edges_c1 = edge_taxa[edge_taxa[level] == c_1]\n",
    "            edges_c2 = edge_taxa[edge_taxa[level] == c_2]\n",
    "            edge_nums1 = np.random.choice(edges_c1.index, size=(4,), replace=False)\n",
    "            edge_nums2 = np.random.choice(edges_c2.index, size=(4,), replace=False)\n",
    "            cross_dist = dist_df.loc[edge_nums1, edge_nums2].mean().mean()\n",
    "            intra_c1_dist = dist_df.loc[edge_nums1, edge_nums1].mean().mean()\n",
    "            intra_c2_dist = dist_df.loc[edge_nums2, edge_nums2].mean().mean()\n",
    "            assert cross_dist > intra_c1_dist\n",
    "            assert cross_dist > intra_c2_dist\n",
    "        if check_counter > 5:\n",
    "            break\n",
    "\n",
    "# these are what we need to calculate effect sizes on \n",
    "sub_dists = dist_df.loc[edge_taxa.index, edge_taxa.index]\n",
    "sub_pooled_pp_ns = pooled_pp_ns.loc[:, edge_taxa.index]\n",
    "\n",
    "edge_names = {x:\"Edge{}\".format(x) for x in list(edge_taxa.index)}\n",
    "\n",
    "to_write_dists = sub_dists.rename(index=edge_names, columns=edge_names)\n",
    "to_write_abunds = rare_pp.rename(columns=edge_names)\n",
    "\n",
    "to_write_dists.to_csv(\"../otu_data/clustered_sequences/fixed_pplacer_distmat.tsv\", sep=\"\\t\")\n",
    "to_write_abunds.to_csv(\"../otu_data/clustered_sequences/pplacer_abundances.tsv\", sep=\"\\t\")\n",
    "\n",
    "meta_data_df.loc[sub_pooled_pp_ns.index, ['CollectionAgency']].to_csv(\"../otu_data/clustered_sequences/strata.tsv\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as mpatches\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "from skbio.stats.distance import permanova\n",
    "from skbio.stats.distance import permdisp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load metadata and filter out OTUs w/ >50% total abundance in blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check effect of rarefaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_df = pd.concat([alpha_df, meta_df_nocodes.loc[alpha_df.index, :]], axis=1)\n",
    "super_exp = super_df[(super_df.station != 'LAB') & (super_df.depth != 'Control') & (super_df.depth != 'nan')]\n",
    "super_exp.loc[super_exp.depth == 'Surface', 'depth'] = '01'\n",
    "super_exp.depth = super_exp.depth.apply(lambda x: \"0\"+x if len(x) == 1 else x)\n",
    "super_sorted = super_exp.sort_values(['year', 'month', 'lat', 'depth' ], ascending=[True, True, False, True])\n",
    "super_sorted['month_year'] = super_sorted.loc[:, ['month', 'year']].apply(lambda x: \" \".join(x), axis=1)\n",
    "super_sorted['salinity_group'] = pd.Series([\"\"]*super_sorted.index.shape[0], index=super_sorted.index)\n",
    "\n",
    "oos_before = abund_df.apply(observed_otus, axis=1) \n",
    "oos_after = rare_abund.apply(observed_otus, axis=1)\n",
    "print(\"Spearman correleations between trimmed read count and observed otus before rarefaction\")\n",
    "print(spearmanr(oos_before.loc[super_sorted.index].values, super_sorted.TrimCount.values))\n",
    "print(\"Spearman correleations between trimmed read count and observed otus after rarefaction\")\n",
    "print(spearmanr(oos_after.loc[super_sorted.index].values, super_sorted.TrimCount.values))\n",
    "print(\"Spearman correleations between trimmed read count and ENSPIE after rarefaction\")\n",
    "print(spearmanr(super_sorted.enspie.values, super_sorted.TrimCount.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha Diversity Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_dict = {\"oligohaline\": ['CB22', 'CB31'],\n",
    "            \"mesohaline\": ['CB32', 'CB33C', 'CB41C', 'CB42C', 'CB43C', 'CB44', 'CB51', 'CB52', 'CB53', 'CB54'],\n",
    "            \"polyhaline\": ['CB61', 'CB62', 'CB63', 'CB64', 'CB71', 'CB72', 'CB73', 'CB74']}\n",
    "\n",
    "for sal_i, stat_ls in sal_dict.items():\n",
    "    super_sorted.loc[super_sorted.station.isin(stat_ls), 'salinity_group'] = sal_i\n",
    "\n",
    "transect_data = super_sorted[super_sorted.station != 'CB33C']\n",
    "cb_33_data = super_sorted[super_sorted.station == 'CB33C']\n",
    "\n",
    "no_outliers = super_sorted[~super_sorted.index.isin(['SB072215TAWCSCB33CD9BR1TR1I36', 'SB072215TAWCSCB33CD6BR1TR1I33'])]\n",
    "for vargrp in ['year', 'month', 'month_year', 'pycnocline', 'salinity_group']:\n",
    "    print(vargrp);\n",
    "    print(super_sorted.loc[:, ['enspie', 'observed_otus', vargrp]].groupby([vargrp]).mean().sort_index())\n",
    "    print(super_sorted.loc[:, ['enspie', 'observed_otus', vargrp]].groupby([vargrp]).std().sort_index())\n",
    "    print(super_sorted.loc[:, ['enspie', 'observed_otus', vargrp]].groupby([vargrp]).count().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel 1 of Alpha Diversity Figure (1): CB33C Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmax1 = (cb_33_data.enspie.max()+cb_33_data.enspie_975.max())*1.1\n",
    "xmax2 = (cb_33_data.observed_otus.max()+cb_33_data.observed_otus_975.max())*1.1\n",
    "plt.clf()\n",
    "fig, ax_arr = plt.subplots(nrows=1, ncols=3, figsize=(8.5,11), dpi=250)\n",
    "height_ = 0.45\n",
    "fig.text(0.5, 0.06, 'Effective Number of Species (ENSPIE)', ha='center')\n",
    "fig.text(0.5, 0.94, \"Observed OTUs\", ha='center')\n",
    "for ar_, yr_ in enumerate(cb_33_data.year.unique()):\n",
    "    sub_df_pre = cb_33_data[cb_33_data.year == yr_].copy().sort_values(['date_float', 'depth'], ascending=False)\n",
    "    sub_df = sub_df_pre.reset_index()\n",
    "    y_reverse = sub_df.index.values #(sub_df.index.values - max(sub_df.index.values))*-1\n",
    "    labs =  sub_df.loc[:, ['month', 'depth']].apply(lambda x: x[0]+\" \"+x[1]+\"m\", axis=1)\n",
    "    ax_arr[ar_].set_title(yr_)\n",
    "    ax_arr[ar_].barh(y=y_reverse-height_, width=sub_df.loc[y_reverse, 'enspie'].values, height=height_,\n",
    "                     xerr=sub_df.loc[:, ['enspie_25', 'enspie_975']].values.T,  color=\"#FF1493\",\n",
    "                     tick_label=labs.values, label=\"ENSPIE\")\n",
    "    ax_arr[ar_].set_ylim([ax_arr[ar_].get_ylim()[0]+1.8, ax_arr[ar_].get_ylim()[1]-1.8])\n",
    "    ax_arr[ar_].set_xlim([0., xmax1])\n",
    "    ax_arr[ar_].set_facecolor('#DCDCDC')\n",
    "    ar_2 = ax_arr[ar_].twiny()\n",
    "    ar_2.barh(y=y_reverse, width=sub_df.loc[y_reverse, 'observed_otus'].values,\n",
    "              xerr=sub_df.loc[:, ['observed_otus_25', 'observed_otus_975']].values.T, \n",
    "              color=\"#00BFFF\", height=height_, label=\"Observed OTUs\")\n",
    "    ar_2.set_xlim([0., xmax2])\n",
    "\n",
    "    ar_2.grid(b=True, which='both', axis='x', color=\"#000000\")\n",
    "    if ar_ == 0:\n",
    "        fig.legend(loc='lower center')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.4)\n",
    "fig.savefig(\"../data/otu_data_pca/alpha_scatter_cb33.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel 2 of Alpha Diveristy Figure (1): Transect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_colors = {'mesohaline':'#DCDCDC',\n",
    "               'oligohaline':'#FFFFFF',\n",
    "               'polyhaline':'#A9A9A9'}\n",
    "t_col_1 = ['CB22', 'CB31', 'CB32', 'CB41C', 'CB42C', 'CB43C', 'CB44']\n",
    "t_col_2 = ['CB51', 'CB52', 'CB53', 'CB54', 'CB61', 'CB62']\n",
    "t_col_3 = ['CB63', 'CB64', 'CB71', 'CB72', 'CB73', 'CB74']\n",
    "\n",
    "height_2 = 0.3\n",
    "xmaxt1 = (transect_data.enspie.max()+transect_data.enspie_975.max())*1.1\n",
    "xmaxt2 = (transect_data.simpson_e.max()+transect_data.simpson_e_975.max())*1.1\n",
    "\n",
    "for col_nstr, stat_grp in enumerate([t_col_1, t_col_2, t_col_3]):\n",
    "    plt.clf()\n",
    "    subdf = transect_data[transect_data.station.isin(stat_grp)]\n",
    "    hrs = [subdf[subdf.station == i].shape[0] for i in stat_grp]\n",
    "    if col_nstr == 2:\n",
    "        hrs.append(hrs[-1])\n",
    "    \n",
    "    fig2 = plt.figure(constrained_layout=True, figsize=(2.83,11), dpi=250)\n",
    "    gs = gridspec.GridSpec(len(hrs), 1, height_ratios=hrs, figure=fig2)\n",
    "    \n",
    "    for facet in range(gs._nrows):\n",
    "        if (col_nstr == 2) and (facet == (gs._nrows-1)):\n",
    "            facax = plt.subplot(gs[facet])\n",
    "            facax.axis('off')\n",
    "        else:\n",
    "            this_stat = stat_grp[facet]\n",
    "            subdubdf_pre = subdf.loc[subdf.station == this_stat].copy().sort_values(['depth', 'date_float'], ascending=False)\n",
    "            subdubdf = subdubdf_pre.reset_index()\n",
    "            y_pos = subdubdf.index.values\n",
    "            labs =  subdubdf.loc[:, ['month', 'depth', 'year']].apply(lambda x: x[0]+\"/\"+x[2][-2:]+\" \"+x[1]+\"m\", axis=1)\n",
    "            facax = plt.subplot(gs[facet])\n",
    "            facax.set_facecolor(face_colors[subdubdf.salinity_group.values[0]])\n",
    "            facax.barh(y=y_pos-height_2, width=subdubdf.loc[:, 'enspie'].values, height=height_2,\n",
    "                       xerr=subdubdf.loc[:, ['enspie_25', 'enspie_975']].values.T,  color=\"#FF1493\",\n",
    "                       tick_label=labs.values, label=\"ENSPIE\")\n",
    "            facax.set_xlim([0., xmaxt1])\n",
    "            facax_2 = facax.twiny()\n",
    "            facax_2.barh(y=y_pos, width=subdubdf.loc[:, 'observed_otus'].values,\n",
    "                         xerr=subdubdf.loc[:, ['observed_otus_25', 'observed_otus_975']].values.T, \n",
    "                         color=\"#00BFFF\", height=height_2, label=\"Observed OTUs\")\n",
    "            facax_2.set_xlim([0., xmax2])\n",
    "            facax_2.grid(b=True, which='both', axis='x', color=\"#000000\")\n",
    "            facax_2.add_artist(AnchoredText(this_stat, borderpad=0., prop=dict(size=12), frameon=False, loc='center right'))\n",
    "            \n",
    "            if facet != (gs._nrows - 1) and (col_nstr != 2):\n",
    "                # the bottom one only retains the enspie labels\n",
    "                facax.set_xticklabels([])\n",
    "            elif facet != (gs._nrows - 2) and (col_nstr == 2):\n",
    "                facax.set_xticklabels([])\n",
    "            elif facet == (gs._nrows - 1) and (col_nstr == 1):\n",
    "                facax.set_xlabel('Effective # of Sp. (ENSPIE)')\n",
    "            \n",
    "            if facet != 0:\n",
    "                # the top one only has simpsons labels\n",
    "                facax_2.set_xticklabels([])\n",
    "            elif (facet == 0) and (col_nstr == 1):\n",
    "                facax_2.set_xlabel('Observed OTUs')\n",
    "            \n",
    "            if this_stat == 'CB63':\n",
    "                fig2.legend(loc='lower center')\n",
    "    \n",
    "    fig2.savefig(\"../data/otu_data_pca/alpha_scatter_transect_{}.png\".format(col_nstr), dpi=250)\n",
    "\n",
    "plt.clf(); plt.close();\n",
    "\n",
    "fig3 = plt.figure(figsize=(8.5, 11), dpi=250)\n",
    "for i in range(3):\n",
    "    img=mpimg.imread(\"../data/otu_data_pca/alpha_scatter_transect_{}.png\".format(i))\n",
    "    sub = fig3.add_subplot(1, 3, i + 1)\n",
    "    sub.axis('off')\n",
    "    sub.imshow(img)\n",
    "\n",
    "fig3.tight_layout()\n",
    "fig3.subplots_adjust(wspace=0)\n",
    "fig3.savefig(\"../data/otu_data_pca/alpha_scatter_transect.png\", dpi=250)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract interesting subclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_ = hierarchy.fclusterdata(projected, c_height, criterion='distance', method='ward', metric='euclidean')\n",
    "clust_cols = ['nMonths', 'nYears', 'nStations', 'nDates', 'nPyc']\n",
    "clust_cols += ['nMonths_noCB33', 'nYears_noCB33', 'nStations_noCB33', 'nDates_noCB33', 'nPyc_noCB33']\n",
    "\n",
    "cluster_stats = pd.DataFrame(index=list(set(list(clustered_))), columns = clust_cols)\n",
    "\n",
    "bottom_only = ['CB22', 'CB31', 'CB32', 'CB52', 'CB51', 'CB43C', 'CB42C', 'CB41C']\n",
    "\n",
    "for x in set(list(clustered_)):\n",
    "    mixed_clust = super_sorted.index[clustered_ == x]\n",
    "    subdf = super_sorted.loc[mixed_clust, :]\n",
    "    num_samps = mixed_clust.shape[0]\n",
    "    print(\"Cluster {} with {} samps\".format(x, num_samps))\n",
    "    print(subdf.enspie.mean(), subdf.enspie.std())\n",
    "    print(subdf.loc[subdf.station.isin(['CB71','CB72', 'CB63', 'CB61', 'CB62', 'CB22']), ['station', 'month_year']])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for significant covariates that obey homogeniety of variance assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_abund2 = rare_abund.loc[super_sorted.index, rare_abund.columns[rare_abund.sum(0) > 0]]\n",
    "exp_abunds2 = exp_abunds.loc[:, exp_abunds.columns[exp_abunds.sum(0) > 0]]\n",
    "print(rare_abund2.shape, exp_abunds2.shape)\n",
    "\n",
    "stat_cols = ['subset_var', 'effect_type', 'nclasses', 'nsamples', 'permanovaF', 'permanovaP', 'dispF', \"dispP\"]\n",
    "results_df = pd.DataFrame(index=range(1000), columns=stat_cols)\n",
    "    \n",
    "for a_df, a_label in zip([exp_abunds2], ['no_rare']):\n",
    "    rclr_mat = rclr().fit_transform(a_df.values)\n",
    "    U, s, V = OptSpace().fit_transform(rclr_mat)\n",
    "    dist_df = pd.DataFrame(index=a_df.index, columns=a_df.index, data=cdist(U,U))\n",
    "    counter = 0\n",
    "    for stat in super_sorted.station.unique():\n",
    "        sub_setter = super_sorted.station == stat\n",
    "        sub_dists = dist_df.loc[sub_setter, sub_setter]\n",
    "        skb_sub_dists = DistanceMatrix(sub_dists.values)\n",
    "        for m_type in ['month_year', 'month', 'year', 'pycnocline']:\n",
    "            mdata = list(super_sorted.loc[sub_setter, m_type].values)\n",
    "            print(\"Checking for {} effects within {}: {} unique classes\".format(stat, m_type, len(set(mdata))))\n",
    "            if len(set(mdata)) > 1:\n",
    "                anova_test = permanova(skb_sub_dists, mdata, permutations=999)\n",
    "                disp_test = permdisp(skb_sub_dists, mdata, permutations=999)\n",
    "                results_df.loc[counter, 'subset_var'] = stat\n",
    "                results_df.loc[counter, 'effect_type'] = m_type\n",
    "                results_df.loc[counter, 'nclasses'] = disp_test['number of groups']\n",
    "                results_df.loc[counter, 'nsamples'] = disp_test['sample size']\n",
    "                results_df.loc[counter, 'permanovaF'] = anova_test['test statistic']\n",
    "                results_df.loc[counter, 'permanovaP'] = anova_test['p-value']\n",
    "                results_df.loc[counter, 'dispF'] = disp_test['test statistic']\n",
    "                results_df.loc[counter, 'dispP'] = disp_test['p-value']\n",
    "                counter += 1\n",
    "                print(\"Counter moving to {}\".format(counter))\n",
    "            else:\n",
    "                print(\"\\t..... skipping\")\n",
    "    \n",
    "    bool1 = super_sorted.station != 'CB33C'\n",
    "    for b2_val in [['2016'], ['2017'], ['2016', '2017']]:\n",
    "        bool2 = super_sorted.year.isin(b2_val)\n",
    "        yearly_transects = DistanceMatrix(dist_df.loc[(bool1 & bool2), (bool1 & bool2)].values)\n",
    "        mdata = list(super_sorted.loc[(bool1 & bool2), 'station'].values)\n",
    "        anova_test = permanova(yearly_transects, mdata, permutations=999)\n",
    "        disp_test = permdisp(yearly_transects, mdata, permutations=999)\n",
    "        results_df.loc[counter, 'subset_var'] = '{}_transect'.format(\"_\".join(b2_val))\n",
    "        results_df.loc[counter, 'effect_type'] = 'Station'\n",
    "        results_df.loc[counter, 'nclasses'] = disp_test['number of groups']\n",
    "        results_df.loc[counter, 'nsamples'] = disp_test['sample size']\n",
    "        results_df.loc[counter, 'permanovaF'] = anova_test['test statistic']\n",
    "        results_df.loc[counter, 'permanovaP'] = anova_test['p-value']\n",
    "        results_df.loc[counter, 'dispF'] = disp_test['test statistic']\n",
    "        results_df.loc[counter, 'dispP'] = disp_test['p-value']\n",
    "        counter += 1\n",
    "        print(\"Counter moving to {}\".format(counter))\n",
    "\n",
    "    for a_stat in super_sorted.station.unique():\n",
    "        bool1 = super_sorted.station == a_stat\n",
    "        for b2_val in ['Above', 'Below']:\n",
    "            bool2 = super_sorted.pycnocline == b2_val\n",
    "            for effect_var in ['month_year', 'month', 'year']:\n",
    "                mdata = list(super_sorted.loc[(bool1 & bool2), effect_var].values)\n",
    "                print(\"Checking for {} effects within {}: {} unique classes\".format(stat, m_type, len(set(mdata))))\n",
    "                if len(set(mdata)) > 1:\n",
    "                    skb_sub_dists = DistanceMatrix(dist_df.loc[(bool1 & bool2), (bool1 & bool2)].values)\n",
    "                    anova_test = permanova(skb_sub_dists, mdata, permutations=999)\n",
    "                    disp_test = permdisp(skb_sub_dists, mdata, permutations=999)\n",
    "                    results_df.loc[counter, 'subset_var'] = a_stat+'_'+b2_val\n",
    "                    results_df.loc[counter, 'effect_type'] = effect_var\n",
    "                    results_df.loc[counter, 'nclasses'] = disp_test['number of groups']\n",
    "                    results_df.loc[counter, 'nsamples'] = disp_test['sample size']\n",
    "                    results_df.loc[counter, 'permanovaF'] = anova_test['test statistic']\n",
    "                    results_df.loc[counter, 'permanovaP'] = anova_test['p-value']\n",
    "                    results_df.loc[counter, 'dispF'] = disp_test['test statistic']\n",
    "                    results_df.loc[counter, 'dispP'] = disp_test['p-value']\n",
    "                    print(\"Counter moving to {}\".format(counter))\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    print(\"\\t..... skipping\")\n",
    "                        \n",
    "\n",
    "    results_df['Real Effect'] = (results_df.permanovaP < 0.05) & (results_df.dispP > 0.05)\n",
    "    results_df['Confounded Effects'] = (results_df.permanovaP < 0.05) & (results_df.dispP < 0.05)\n",
    "    results_df['No Effects'] = (results_df.permanovaP > 0.05) & (results_df.dispP > 0.05)\n",
    "    results_df.sort_values(by=['Real Effect', 'Confounded Effects', 'No Effects'], ascending=False, inplace=True)\n",
    "    results_df[results_df.subset_var.notnull()].to_csv('../data/otu_data_pca/hypothesis_testing_{}.tsv'.format(a_label), sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_f = \"../data/TrimOTUsData/taxa_table.tsv\"\n",
    "taxa_df = pd.read_csv(tax_f, sep=\"\\t\")\n",
    "OTU_Seqs = {taxa_df.loc[idx, taxa_df.columns[0]]:idx for idx in taxa_df.index}\n",
    "OTU_Names = {idx:\"OTU{}\".format(idx+1) for idx in taxa_df.index }\n",
    "OTU_name2seq = {OTU_Names[num]:seq for seq, num in OTU_Seqs.items()}\n",
    "taxa_df.loc[:, taxa_df.columns[0]] = taxa_df.loc[:, taxa_df.columns[0]].apply(lambda x: OTU_Names[OTU_Seqs[x]])\n",
    "taxa_df = taxa_df.set_index(taxa_df.columns[0])\n",
    "\n",
    "assert str(taxa_df_light.iloc[0, -1]) == 'nan'\n",
    "taxa_df_light = taxa_df.loc[abund_df.columns, :]\n",
    "taxa_df_ln = taxa_df_light.replace(taxa_df_light.iloc[0, -1], \"\")\n",
    "for c in taxa_df_ln.columns:\n",
    "    taxa_df_ln[c] = taxa_df_ln[c].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_genera = {}\n",
    "with open(\"../data/Problem_Taxa.txt\", 'r') as pt_fh:\n",
    "    for l in pt_fh:\n",
    "        pbg = l.split()[0].lower()\n",
    "        if pbg in problem_genera.keys():\n",
    "            pass\n",
    "        else:\n",
    "            flagged_idxs = set()\n",
    "            if pbg != 'candida':\n",
    "                flagged_idxs.update(taxa_df_ln[taxa_df_ln.Family.str.contains(pbg)].index)\n",
    "                flagged_idxs.update(taxa_df_ln[taxa_df_ln.Genus.str.contains(pbg)].index)\n",
    "            else:\n",
    "                flagged_idxs.update(taxa_df_ln[taxa_df_ln.Family.str.contains(pbg) & ~taxa_df_ln.Family.str.contains('candidatus')].index)\n",
    "                flagged_idxs.update(taxa_df_ln[taxa_df_ln.Genus.str.contains(pbg) & ~taxa_df_ln.Genus.str.contains('candidatus')].index)\n",
    "            \n",
    "            if len(flagged_idxs) > 0:\n",
    "                problem_genera[pbg] = list(flagged_idxs)\n",
    "                print(\"{}: {} unique types\".format(len(flagged_idxs), pbg))\n",
    "                print(taxa_df_ln.loc[problem_genera[pbg], :].drop_duplicates())\n",
    "                input()\n",
    "\n",
    "\n",
    "taxa_df_fgs = taxa_df_ln.loc[:, ['Family', \"Genus\", \"Species\"]].apply(tuple, axis=1)\n",
    "\n",
    "med_detected = [(\"neisseriaceae\", 'neisseria', \"\"),\n",
    "                (\"pseudomonadaceae\", 'pseudomonas', \"\"), \n",
    "                (\"spirochaetaceae\", \"treponema\", \"\"), \n",
    "                (\"spirochaetaceae\", \"treponema_2\", \"\"),\n",
    "                (\"rickettsiaceae\", \"rickettsia\", \"\"), \n",
    "                (\"rickettsiaceae\", \"rickettsia\", \"typhi\"), \n",
    "                (\"leptospiraceae\", \"leptospira\", \"\"),\n",
    "                (\"legionellaceae\", \"legionella\", \"\"),\n",
    "                (\"legionellaceae\", \"legionella\", \"steelei\"),\n",
    "                (\"francisellaceae\", \"francisella\", \"\"),\n",
    "                (\"pasteurellaceae\", \"haemophilus\", \"\"),\n",
    "                (\"parachlamydiaceae\", \"parachlamydia\", \"acanthamoebae\"),\n",
    "                (\"aeromonadaceae\", \"aeromonas\", \"\"),\n",
    "                (\"coxiellaceae\", \"coxiella\", \"cheraxi\"),\n",
    "                (\"coxiellaceae\", \"coxiella\", \"\"),\n",
    "                (\"enterobacteriaceae\", \"yersinia\", \"\"),\n",
    "                (\"vibrionaceae\", \"vibrio\", \"\"),\n",
    "                (\"peptostreptococcaceae\", \"peptoclostridium\", \"\"),\n",
    "                (\"peptostreptococcaceae\", \"paeniclostridium\", \"\"),\n",
    "                (\"clostridiaceae_2\", \"clostridium_sensu_stricto\", \"\"),\n",
    "                (\"clostridiaceae_1\", \"clostridium_sensu_stricto_3\", \"intestinale\"),\n",
    "                (\"bacteroidaceae\", \"bacteroides\", \"\"),\n",
    "                (\"bacteroidaceae\", \"bacteroides\", \"vulgatus\"),\n",
    "                (\"bacteroidaceae\", \"bacteroides\", \"uniformis\"),\n",
    "                (\"bacteroidaceae\", \"bacteroides\", \"coprosuis\"),\n",
    "                (\"bacteroidaceae\", \"bacteroides\", \"massiliensis\"),\n",
    "                (\"campylobacteraceae\", \"campylobacter\", \"\"),\n",
    "                (\"listeriaceae\", \"listeria\", \"\"),\n",
    "                (\"enterobacteriaceae\", \"escherichia/shigella\", \"\"),\n",
    "                (\"mycobacteriaceae\", \"mycobacteriummycobacteriaceae\", \"\"),\n",
    "                (\"moraxellaceae\", \"acinetobacter\", \"\"),\n",
    "                (\"streptococcaceae\", \"streptococcus\", \"mutans\"),\n",
    "                (\"streptococcaceae\", \"streptococcus\", \"\"),\n",
    "                (\"peptostreptococcaceae\", \"peptostreptococcus\", \"\"),\n",
    "                (\"enterococcaceae\", \"enterococcus\", \"\"),\n",
    "                (\"enterobacteriaceae\", \"serratia\", \"\"),\n",
    "                (\"enterobacteriaceae\", \"klebsiella\", \"\"),\n",
    "                (\"enterobacteriaceae\", \"salmonella\", \"\"),\n",
    "                (\"enterobacteriaceae\", \"citrobacter\", \"\"),\n",
    "                (\"enterobacteriaceae\", \"pantoea\", \"\"),\n",
    "                (\"lactobacillaceae\", \"lactobacillus\", \"delbrueckii\"),\n",
    "                (\"lactobacillaceae\", \"lactobacillus\", \"kitasatonis\"),\n",
    "                (\"staphylococcaceae\", \"staphylococcus\", \"haemolyticus\"),\n",
    "                (\"staphylococcaceae\", \"staphylococcus\", \"\"),\n",
    "                (\"bifidobacteriaceae\",  \"bifidobacterium\", \"bifidum\"),\n",
    "                (\"bifidobacteriaceae\",  \"bifidobacterium\", \"\")]\n",
    "\n",
    "hab_detected = [(\"nostocaceae\", \"aphanizomenon_mdt14a\", \"\"),\n",
    "                (\"cyanobiaceae\", \"cyanobium_pcc-6307\", \"\"),\n",
    "                (\"nostocaceae\", \"cylindrospermum_pcc-7417\", \"\"),\n",
    "                (\"nostocaceae\", \"dolichospermum_nies41\", \"\"),\n",
    "                (\"limnotrichaceae\", \"limnothrix\", \"\"),\n",
    "                (\"microcystaceae\", \"microcystis_pcc-7914\", \"\"),\n",
    "                (\"nostocaceae\", \"nodularia_pcc-9350\", \"\"),\n",
    "                (\"nostocales_incertae_sedis\", \"phormidium_sag_81.79\", \"uncinatum\"),\n",
    "                (\"phormidiaceae\", \"phormidium_iam_m-71\", \"\"),\n",
    "                (\"phormidiaceae\", \"planktothrix_niva-cya_15\", \"\"),\n",
    "                (\"microcystaceae\", \"snowella_0tu37s04\", \"\"),\n",
    "                (\"microcystaceae\",  \"microcystis_pcc-7914\", \"\"),\n",
    "                (\"microcystaceae\", \"snowella_0tu37s04\", \"litoralis\")]\n",
    "\n",
    "for i in range(1,19):\n",
    "    med_detected.append((\"clostridiaceae_1\", \"clostridium_sensu_stricto_\"+str(i), \"\"))\n",
    "\n",
    "med_idxs = taxa_df_fgs[taxa_df_fgs.isin(med_detected)].index\n",
    "hab_idxs = taxa_df_fgs[taxa_df_fgs.isin(hab_detected)].index\n",
    "\n",
    "\n",
    "print(med_idxs.shape, hab_idxs.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df1 = abund_df_c2.loc[ordered_axis, med_idxs]\n",
    "plot_df2 = abund_df_c2.loc[ordered_axis, hab_idxs]\n",
    "#sns.set(font_scale=.5)\n",
    "plt.clf(); plt.close();\n",
    "plt.style.use('seaborn-paper')\n",
    "fig_, (ax_1, ax_2) = plt.subplots(nrows=1, ncols=2, sharey='col', figsize=(60,60), dpi=180, gridspec_kw = {'width_ratios':[5.7, 1]})\n",
    "sns.heatmap(plot_df1, cmap=sns.light_palette('red', as_cmap=True), robust=True, linewidths=.5, ax=ax_1, cbar=False)\n",
    "sns.heatmap(plot_df2, cmap=sns.light_palette('green', as_cmap=True), robust=True, linewidths=.5, ax=ax_2, cbar=False)\n",
    "matplot_fn = \"../data/WaterQualityData/figures/probTaxa.png\"\n",
    "fig_.savefig(matplot_fn, dpi=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the usearch data\n",
    "c90_file = \"../otu_data/clustered_sequences/abundances.c90.tsv\"\n",
    "if not os.path.exists(c90_file):\n",
    "    cluster_mem_file = \"../otu_data/clustered_sequences/cluster_members.90.txt\"\n",
    "    clust90 = pd.read_csv(cluster_mem_file, sep=\"\\t\", header=None)\n",
    "    \n",
    "    def cluster_table(clustxx, abund_df_i):\n",
    "        c_labels = ['Cluster'+str(c_i) for c_i in clustxx[1].unique()]\n",
    "        clust_xx_df = pd.DataFrame(index=abund_df_i.index, columns=c_labels)\n",
    "        clust_dict = {}\n",
    "        for c_labs in c_labels:\n",
    "            c_int = int(c_labs[7:])\n",
    "            clust_dict[c_int] = list(set(clustxx[clustxx[1].isin([c_int])][8].values))\n",
    "            clust_xx_df.loc[:, c_labs] = abund_df_i.loc[:, abund_df_i.columns.isin(clust_dict[c_int])].sum(1)\n",
    "        \n",
    "        return (clust_xx_df, clust_dict)\n",
    "    \n",
    "    clust_90_df, clust_90_dict = cluster_table(clust90, abund_df_jm.copy())\n",
    "    print(\"After clustering {} features reduced to {}\".format(abund_df_jm.shape[1], clust_90_df.shape[1]))\n",
    "    assert clust_90_df.sum().sum() - abund_df_jm.sum().sum() == 0\n",
    "    clust_90_df.to_csv(c90_file, sep=\"\\t\")\n",
    "    print(\"Clustered file written\")\n",
    "else:\n",
    "    print(\"Reading stored UCLUST abundances\")\n",
    "    clust_90_df = pd.read_csv(c90_file, sep=\"\\t\", index_col=0)\n",
    "    print(\"Decreasing sparsity of clustered abundances\")    \n",
    "\n",
    "clust_90_ns = decrease_sparsity(clust_90_df.copy(), control_libs, addl_keys=['Zymo'])\n",
    "print(\"After clustering {} features reduced to {}\".format(abund_df_jm.shape[1], clust_90_ns.shape[1]))\n",
    "\n",
    "from skbio.stats.distance import mantel\n",
    "\n",
    "assert clust_90_ns.index.equals(abund_df_og.index)\n",
    "assert pooled_pp_ns.index.equals(abund_df_og.index)\n",
    "\n",
    "bc_dm_pp = beta_diversity(\"braycurtis\", pooled_pp_ns.values, pooled_pp_ns.index)\n",
    "bc_dm_c90 = beta_diversity(\"braycurtis\", clust_90_ns.values, clust_90_ns.index)\n",
    "bc_dm_og = beta_diversity(\"braycurtis\", abund_df_og.values, abund_df_og.index)\n",
    "\n",
    "r_pp, p_value_pp, n_pp = mantel(bc_dm_pp, bc_dm_og, method='pearson')\n",
    "r_c90, p_value_c90, n_c90 = mantel(bc_dm_pp, bc_dm_c90, method='pearson')\n",
    "print(\"Correlation of UCLUST 90% to original {} ({})\".format(r_c90, p_value_c90))\n",
    "print(\"Correlation of Pplacer edges to original {} ({})\".format(r_pp, p_value_pp))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
