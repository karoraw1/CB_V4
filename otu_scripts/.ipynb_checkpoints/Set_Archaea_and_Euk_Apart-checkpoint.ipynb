{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Alphabet import generic_dna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxa_file = \"taxa_table.tsv\"\n",
    "abund_file = \"abundance_table.tsv\"\n",
    "data_path = \"../otu_data/dada2_outputs\"\n",
    "out_path = \"../otu_data/tree_data\"\n",
    "abund_f = os.path.join(data_path, abund_file)\n",
    "tax_f = os.path.join(data_path, taxa_file)\n",
    "taxa_df = pd.read_csv(tax_f, sep=\"\\t\")\n",
    "abund_df = pd.read_csv(abund_f, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Renaming rows from the entire sequences to OTU# format\")\n",
    "print(\"\\tStoring sequences in dictionary, accesible by OTU name\")\n",
    "OTU_Seqs = {taxa_df.loc[idx, taxa_df.columns[0]]:idx for idx in taxa_df.index}\n",
    "OTU_Names = {idx:\"OTU{}\".format(idx+1) for idx in taxa_df.index }\n",
    "OTU_name2seq = {OTU_Names[num]:seq for seq, num in OTU_Seqs.items()}\n",
    "taxa_df.loc[:, taxa_df.columns[0]] = taxa_df.loc[:, taxa_df.columns[0]].apply(lambda x: OTU_Names[OTU_Seqs[x]])\n",
    "taxa_df = taxa_df.set_index(taxa_df.columns[0])\n",
    "new_cols = ['Samples']+[OTU_Names[OTU_Seqs[y]] for y in abund_df.columns[1:]]\n",
    "abund_df.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abund_df2 = abund_df.set_index('Samples')\n",
    "abund_df3 = abund_df2[abund_df2.sum(1)!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abund_df2.shape, abund_df3.shape)\n",
    "\n",
    "with open(os.path.join(out_path, 'cov_model_data', 'poor_aligners.txt')) as paFH:\n",
    "    poor_aligners = [i for i in paFH.read().split(\"\\n\") if i != \"\"]\n",
    "    \n",
    "with open(os.path.join(out_path, 'cov_model_data', 'reverse_strand_aligners.txt')) as rsFH:\n",
    "    rev_strand_algn = [i for i in rsFH.read().split(\"\\n\") if i != \"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_taxa = set(poor_aligners)\n",
    "print(len(pa_taxa))\n",
    "length_filter = set([i for i in OTU_name2seq.keys() if len(OTU_name2seq[i]) < 240 or len(OTU_name2seq[i]) > 260])\n",
    "print(len(length_filter))\n",
    "arch_euk = set(taxa_df.index[taxa_df.Kingdom.isin(['Archaea', 'Eukaryota'])])\n",
    "print(len(arch_euk))\n",
    "print(len(arch_euk.intersection(pa_taxa)))\n",
    "print(len(arch_euk.intersection(length_filter)))\n",
    "to_remove = arch_euk - length_filter.union(pa_taxa)\n",
    "print(len(to_remove))\n",
    "\n",
    "print(set(['OTU47634', 'OTU19288', 'OTU18363']).intersection(set(abund_df3.columns)))\n",
    "print(set(['OTU47634', 'OTU19288', 'OTU18363']).intersection(arch_euk))\n",
    "print(set(['OTU47634', 'OTU19288', 'OTU18363']).intersection(pa_taxa))\n",
    "print(set(['OTU47634', 'OTU19288', 'OTU18363']).intersection(length_filter))\n",
    "print(set(['OTU47634', 'OTU19288', 'OTU18363']).intersection(to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abund_feather = '../otu_data/tree_data/hq_asv_table.feather'\n",
    "abund_dfx = pd.read_feather(abund_feather, use_threads=True).set_index('Samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "been_removed = set(abund_df3.columns) - set(abund_dfx.columns)\n",
    "still_removed = been_removed - to_remove\n",
    "print(len(been_removed), len(still_removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_bacterial_abunds = abund_df3.loc[:, abund_df3.columns.isin(still_removed)].reset_index()\n",
    "print(non_bacterial_abunds.shape)\n",
    "print(non_bacterial_abunds.iloc[:3, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out fresh abundance table\n",
    "# '../otu_data/tree_data/non_bact_table.feather'\n",
    "abund_feather = os.path.join(out_path, 'non_bact_table.feather')\n",
    "non_bacterial_abunds.to_feather(abund_feather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix of bacterial and some archael abundances\n",
    "bacterial_abunds = pd.read_csv(\"../otu_data/abundances_full.txt\", sep=\"\\t\", index_col=0)\n",
    "# everything that was prematurely dropped\n",
    "nba_1_df = non_bacterial_abunds.set_index('Samples').loc[bacterial_abunds.index, :]\n",
    "# relative abundance\n",
    "nba_1_df_ra = nba_1_df.div(nba_1_df.sum(1), axis=0)\n",
    "# only keeping things that exist \n",
    "low_abund = set(nba_1_df.columns[(nba_1_df_ra > 0).sum() == 0])\n",
    "# make pa\n",
    "presence_absence = ((nba_1_df > 0)).astype(int)\n",
    "# only keep things that are present in more than 2 samples \n",
    "low_abund = low_abund.union(set(nba_1_df.columns[presence_absence.sum() < 0]))\n",
    "print(len(low_abund))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_fn = \"../otu_data/final_rarefied_table.tsv\"\n",
    "meta_fn = \"../otu_data/final_metadata.tsv\"\n",
    "meta_df_x = pd.read_csv(meta_fn, sep=\"\\t\", index_col=0, converters={'DateMMDDYY': lambda x: str(x)})\n",
    "rare_abund = pd.read_csv(rare_fn, sep=\"\\t\", index_col=0)\n",
    "env_data = pd.read_csv(\"../otu_data/environmental_and_mapping_data.txt\", sep=\"\\t\", index_col=0)\n",
    "matched_cols = [i for i in meta_df_x.index if i in set(bacterial_abunds.index) and meta_df_x.loc[i, 'DepthName'] != 'LAB']\n",
    "meta_df_x = meta_df_x.loc[matched_cols, :]\n",
    "rare_abund = rare_abund.loc[matched_cols, :]\n",
    "env_data = env_data.loc[matched_cols, :]\n",
    "\n",
    "meta_df_x['Actual Depth (m)'] = pd.Series({i:env_data.loc[i, 'Actual Depth (m)'] for i in meta_df_x.index})\n",
    "meta_df_x['Depth'] = meta_df_x['DepthName'].astype(int)\n",
    "meta_df_x.loc[meta_df_x['Actual Depth (m)'].notnull(), 'Depth'] = meta_df_x.loc[meta_df_x['Actual Depth (m)'].notnull(), 'Actual Depth (m)']\n",
    "\n",
    "full_df = pd.concat([nba_1_df.loc[matched_cols, :], \n",
    "                     bacterial_abunds.loc[matched_cols, :]], axis=1, sort=True)\n",
    "print(full_df.shape)\n",
    "\n",
    "needed_cols = ['CollectionAgency', 'StationName', 'Month', 'Year', 'DateMMDDYY', 'enspie', 'faith_pd',\n",
    "               'Depth', 'Latitude', 'observed_otus']\n",
    "weighted_uf_cols = [i for i in meta_df_x.columns if i.startswith(\"SB\") and i.endswith(\"_wu\")]\n",
    "weighted_bc_cols = [i for i in meta_df_x.columns if i.startswith(\"SB\") and i.endswith(\"_bc\")]\n",
    "weighted_clr_cols = [i for i in meta_df_x.columns if i.startswith(\"SB\") and i.endswith(\"_clr\")]\n",
    "\n",
    "\n",
    "#to_csv(\"../masters_students/\", sep='\\t', index_label='Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_singles_f = \"../otu_data/WaterQualityData/matched_cleaned_data/all_mdata_colset_2.tsv\"\n",
    "cs_df = pd.read_csv(clean_singles_f, sep=\"\\t\", index_col=0).drop(\"depth_float\", axis=1)\n",
    "\n",
    "transect_data_f = \"../otu_data/WaterQualityData/matched_cleaned_data/transect_mdata_colset_1.tsv\"\n",
    "td_df = pd.read_csv(transect_data_f, sep=\"\\t\", index_col=0)\n",
    "\n",
    "for new_col in [i for i in td_df.columns if not i in set(cs_df.columns)]:\n",
    "    cs_df[new_col] = pd.Series({i:td_df.loc[i, new_col] for i in cs_df.index if i in set(td_df.index)})\n",
    "\n",
    "for new_col in ['Depth', 'DateMMDDYY', 'observed_otus']:\n",
    "    cs_df[new_col] = pd.Series({i:meta_df_x.loc[i, new_col] for i in cs_df.index if i in set(meta_df_x.index)})\n",
    "\n",
    "print(cs_df.shape, cs_df.columns)\n",
    "\n",
    "alpha_diversity = ['enspie', 'observed_otus', 'faith_pd']\n",
    "beta_div_uf = [i+\"_wu\" for i in list(cs_df.index)]\n",
    "beta_div_bc = [i+\"_bc\" for i in list(cs_df.index)]\n",
    "env_data_cols = ['Month', 'Year', 'DateMMDDYY', 'CollectionAgency', 'StationName', 'Latitude', 'Depth', \n",
    "                 'Depth_Percentage', 'WTEMP', 'DO', 'CHLA', 'SALINITY','NO2F', 'NH4F', 'PC', 'PHEO', 'NO3F', \n",
    "                 'Discharge_Susquehanna_14', 'day_length']\n",
    "\n",
    "cs_df[alpha_diversity].to_csv(\"/Users/login/Google Drive/SiYi_Xiaotong_Materials/alpha_diversity.txt\", sep='\\t', index_label='Sample')\n",
    "meta_df_x.loc[list(cs_df.index), beta_div_uf].to_csv(\"/Users/login/Google Drive/SiYi_Xiaotong_Materials/bray_curtis_betadiversity.txt\", sep='\\t', index_label='Sample')\n",
    "meta_df_x.loc[list(cs_df.index), beta_div_bc].to_csv(\"/Users/login/Google Drive/SiYi_Xiaotong_Materials/weighted_unifrac_betadiversity.txt\", sep='\\t', index_label='Sample')\n",
    "cs_df[env_data_cols].to_csv(\"/Users/login/Google Drive/SiYi_Xiaotong_Materials/env_metadata.txt\", sep='\\t', index_label='Sample')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxa_df['Species2'] = taxa_df['Genus'] + \" \" + taxa_df['Species']\n",
    "taxas_1 = taxa_df.copy().drop('Species', axis=1).astype(str)\n",
    "\n",
    "print((taxas_1['Order'] == 'Chloroplast').sum())\n",
    "taxas_1.loc[taxas_1['Order'] == 'Chloroplast', 'Kingdom'] = 'Eukaryota'\n",
    "taxas_1.loc[taxas_1['Order'] == 'Chloroplast', 'Phylum'] = 'Chloroplast'\n",
    "taxas_1.loc[taxas_1['Order'] == 'Chloroplast', 'Class'] = 'Chloroplast'\n",
    "taxas_1.loc[taxas_1['Order'] == 'Chloroplast', 'Class'] = 'Chloroplast'\n",
    "taxas_1.loc[taxas_1['Order'] == 'Chloroplast', 'Family'] = 'Chloroplast'\n",
    "\n",
    "print((taxas_1['Family'] == 'Mitochondria').sum())\n",
    "taxas_1.loc[taxas_1['Family'] == 'Mitochondria', 'Kingdom'] = 'Eukaryota'\n",
    "taxas_1.loc[taxas_1['Family'] == 'Mitochondria', 'Phylum'] = 'Mitochondria'\n",
    "taxas_1.loc[taxas_1['Family'] == 'Mitochondria', 'Class'] = 'Mitochondria'\n",
    "taxas_1.loc[taxas_1['Family'] == 'Mitochondria', 'Order'] = 'Mitochondria'\n",
    "taxas_1.loc[taxas_1['Family'] == 'Mitochondria', 'Class'] = 'Mitochondria'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_abundances = full_df.loc[cs_df.index, :]\n",
    "order_df = pd.DataFrame(index=subset_abundances.index, \n",
    "                        columns=[i for i in taxas_1['Order'].astype(str).unique()])\n",
    "\n",
    "for ord_ in sorted(order_df.columns):\n",
    "    print(ord_)    \n",
    "    otus_in_abund_df = set(list(taxas_1.index[taxas_1['Order'].astype(str) == ord_]))\n",
    "    print(len(otus_in_abund_df))\n",
    "    otus_in_abund_df = otus_in_abund_df.intersection(set(subset_abundances.columns))\n",
    "    print(len(otus_in_abund_df))\n",
    "    aggd_data = subset_abundances[otus_in_abund_df].sum(1)\n",
    "    if aggd_data.sum() > 0:\n",
    "        order_df[ord_] = subset_abundances[otus_in_abund_df].sum(1)\n",
    "    else:\n",
    "        order_df.drop(ord_, axis=1, inplace=True)\n",
    "\n",
    "order_df_ra = order_df.div(order_df.sum(1), axis=0)*1e6\n",
    "order_df_ra.to_csv(\"/Users/login/Google Drive/SiYi_Xiaotong_Materials/taxa_order_counts.txt\",\n",
    "                   sep='\\t', index_label='Sample')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxa_breakdown(abunds_, taxas_, level_, weighted=True, flatten_val=0.0):\n",
    "    # 'Kingdom', 'Phylum', 'Class', 'Order', 'Family', 'Genus', 'Species'\n",
    "    # remove non-existant features\n",
    "    flip_abunds = abunds_.loc[:, abunds_.sum(0) > 0].T\n",
    "    # create presence or absence table if need be\n",
    "    if not weighted:\n",
    "        flip_abunds = (flip_abunds > 0).astype(int)\n",
    "    # add level column\n",
    "    otu_fetch = lambda x: taxas_.loc[x, level_]\n",
    "    flip_abunds['otu_name'] = flip_abunds.index\n",
    "    flip_abunds['taxa_name'] = flip_abunds['otu_name'].apply(otu_fetch)\n",
    "    flip_abunds.drop('otu_name', axis=1, inplace=True)\n",
    "    ttable_raw = flip_abunds.groupby('taxa_name').agg(np.sum)\n",
    "    ttable = ttable_raw.div(ttable_raw.sum(0))\n",
    "    if flatten_val:\n",
    "        flat_ttv = ttable.values\n",
    "        flat_ttv[flat_ttv < flatten_val] = 0.0\n",
    "        ttable = pd.DataFrame(flat_ttv, index=ttable.index, columns=ttable.columns)\n",
    "    return ttable.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abunds_1 = subset_abundances.copy()\n",
    "relativize = lambda v: (v.sum() / v.sum().sum()).sort_values(ascending=False)\n",
    "\n",
    "flattened_tables_pa = {}\n",
    "for level_1, fv in zip(taxas_1.columns, [0, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03]):\n",
    "    ttable_1 = taxa_breakdown(abunds_1, taxas_1, level_1, weighted=False, flatten_val=fv)\n",
    "    ttable_1 = ttable_1.loc[:, ttable_1.columns[ttable_1.sum() > 0]]\n",
    "    col_order = ttable_1.max().sort_values(ascending=False).index\n",
    "    ttable_1 = ttable_1.loc[:, col_order]\n",
    "    print(\"The collapsed {} taxa table is {}\".format(level_1, ttable_1.shape))\n",
    "    flattened_tables_pa[level_1] = relativize(ttable_1.copy())\n",
    "    \n",
    "flattened_tables = {}\n",
    "for level_1, fv in zip(taxas_1.columns, [0, 0.05, 0.05, 0.1, 0.08, 0.08, 0.01]):\n",
    "    ttable_1 = taxa_breakdown(abunds_1, taxas_1, level_1, weighted=True, flatten_val=fv)\n",
    "    ttable_1 = ttable_1.loc[:, ttable_1.columns[ttable_1.sum() > 0]]\n",
    "    col_order = ttable_1.max().sort_values(ascending=False).index\n",
    "    ttable_1 = ttable_1.loc[:, col_order]\n",
    "    print(\"The collapsed {} taxa table is {}\".format(level_1, ttable_1.shape))\n",
    "    flattened_tables[level_1] = relativize(ttable_1.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "def plot_taxa(ttable_mix, key_order, height_rat, bar_width_mult, fignamex=None):\n",
    "    plt.clf(); plt.close();\n",
    "    fig_width = 12\n",
    "    fig_t = plt.figure(figsize=(fig_width,10), dpi=140)\n",
    "    gs = gridspec.GridSpec(2, len(key_order), figure=fig_t, height_ratios=height_rat, hspace=.05, wspace=0.05,\n",
    "                           bottom=0.075, top=0.925, right=0.925, left=0.075)\n",
    "    \n",
    "    # set the width of each bar to the number of samples\n",
    "    adjusted_width = (fig_width / len(key_order))*(bar_width_mult)\n",
    "    # set the left bottom anchor of each bar\n",
    "    bar_locs = np.arange(len(key_order))*(fig_width / len(key_order))\n",
    "    for ko_i, ko in enumerate(key_order):\n",
    "        ttable = ttable_mix[ko]\n",
    "        possible_colors = [j for i, j in sns.xkcd_rgb.items() if not 'white' in i]\n",
    "        np.random.seed(2)\n",
    "        colors_needed = np.random.choice(possible_colors, size=ttable.shape)\n",
    "        print(\"{} colors grabbed\".format(len(colors_needed)))\n",
    "\n",
    "        ax_i = plt.subplot(gs[0,ko_i])\n",
    "        # set the bar labels \n",
    "        bar_names = [ko]\n",
    "        # loop over each taxon name\n",
    "        for bar_n, bar_col in enumerate(ttable.index):\n",
    "            # subset those fractions across samples\n",
    "            bar_x = np.array([ttable[bar_col]])\n",
    "            # set the y-axis location for each bar\n",
    "            if bar_n == 0:\n",
    "                running_base = bar_x*0.0\n",
    "            # Create an individual bar\n",
    "            ax_i.bar([bar_locs[ko_i]], bar_x, bottom=running_base, \n",
    "                     color=colors_needed[bar_n], edgecolor='white', \n",
    "                     width=adjusted_width)\n",
    "            for tick in ax_i.get_xticklabels():\n",
    "                tick.set_rotation(45)\n",
    "            # increment the bottoms\n",
    "            running_base = running_base + bar_x\n",
    "        ax_i.axis('off')\n",
    "        \n",
    "        ax2 = plt.subplot(gs[1,ko_i])\n",
    "        patches = [mpatches.Patch(color=color, label=label) for label, color in zip(list(ttable.index), colors_needed)]\n",
    "        ax2.legend(patches, list(ttable.index), loc='best', \n",
    "                   bbox_to_anchor=(0., 0., 1., 1.),\n",
    "                   mode='expand', fontsize='x-small', ncol=1)\n",
    "\n",
    "        ax2.axis('off')\n",
    "    # Show graphic\n",
    "    plt.show()\n",
    "    if fignamex:\n",
    "        fig_t.savefig(fignamex, dpi=140)\n",
    "\n",
    "    return\n",
    "\n",
    "fignamet = \"AllTaxonomy_AllSamples_unweighted.png\".format(level_1)\n",
    "figpatht = os.path.join(\"../otu_data/pca_plots\", fignamet)\n",
    "plot_taxa(flattened_tables_pa, [\"Kingdom\", \"Phylum\", \"Class\", \"Order\", \"Family\"],\n",
    "          [7,4], .9, fignamex=figpatht)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fignamet = \"AllTaxonomy_AllSamples_weighted.png\".format(level_1)\n",
    "figpatht = os.path.join(\"../otu_data/pca_plots\", fignamet)\n",
    "plot_taxa(flattened_tables, [\"Kingdom\", \"Phylum\", \"Class\", \"Order\", \"Family\"],\n",
    "          [7,4], .9, fignamex=figpatht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_to_write = \"\"\n",
    "for i in subset_abundances.columns:\n",
    "    str_to_write += \">\"+i+\"\\n\"\n",
    "    str_to_write += OTU_name2seq[i] + \"\\n\"\n",
    "\n",
    "with open(\"/Volumes/KeithSSD/CB_V4/otu_data/FAPROTAX_out/otu_seqs_for_rdp.fasta\", 'w') as fh:\n",
    "    _ = fh.write(str_to_write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biom.table import Table\n",
    "from biom.util import biom_open\n",
    "\n",
    "name_checks = {'SAR11_clade': 'SAR11 clade',\n",
    "               'SAR86_clade': 'SAR86 clade'}\n",
    "name_corrector = lambda x: name_checks[x] if x in name_checks.keys() else x\n",
    "\n",
    "sample_ids = []\n",
    "for i in list(subset_abundances.index):\n",
    "    sample_ids.append(i)\n",
    "\n",
    "_outname_ = '_silva'\n",
    "observ_ids, observ_metadata = [], []\n",
    "for i in list(subset_abundances.columns):\n",
    "    if i.startswith(\"OTU\") and i in list(taxa_df.index):\n",
    "        observ_ids.append(i)\n",
    "        observ_metadata.append({'taxonomy': [name_corrector(j) for j in taxa_df.loc[i, :].dropna().values]})\n",
    "\n",
    "_data_ = subset_abundances.loc[sample_ids, observ_ids].values.T\n",
    "print(_outname_, _data_.shape, subset_abundances.shape)\n",
    "table = Table(_data_, observ_ids, sample_ids, observ_metadata, None)\n",
    "with biom_open('../otu_data/FAPROTAX_out/otu_taxa{}.biom'.format(_outname_), 'w') as f:  \n",
    "    table.to_hdf5(f, \"faith and trust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdp_taxa = pd.read_csv(\"/Volumes/KeithSSD/CB_V4/otu_data/FAPROTAX_out/fixrank_rdp_classifiedx.txt\", \n",
    "                       sep=\";\", index_col=0, header=None).drop(1, axis=1)\n",
    "rdp_taxa.index.name = ''\n",
    "rdp_cols = ['K', 'K%', 'P', 'P%', 'C', 'C%', 'O', 'O%', 'F', 'F%', 'G', 'G%']\n",
    "rdp_taxa.columns = rdp_cols\n",
    "print(rdp_taxa.shape)\n",
    "rdp_name_cols = [i for i in rdp_cols if not '%' in i]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "for i in rdp_name_cols:\n",
    "    level_pct = rdp_taxa[i+'%'].apply(lambda x: float(x[:-1]) < 80.)\n",
    "    rdp_taxa.loc[level_pct, i] = np.nan\n",
    "    rdp_taxa.drop(i+'%', axis=1, inplace=True)\n",
    "    print(\"{}: {}\".format(i, rdp_taxa[i].notnull().sum()))\n",
    "\n",
    "_outname_ = '_rdp'\n",
    "observ_ids, observ_metadata = [], []\n",
    "for i in list(subset_abundances.columns):\n",
    "    if i.startswith(\"OTU\") and i in list(rdp_taxa.index):\n",
    "        observ_ids.append(i)\n",
    "        observ_metadata.append({'taxonomy': [name_corrector(j) for j in rdp_taxa.loc[i, :].dropna().values]})\n",
    "        \n",
    "_data_ = subset_abundances.loc[sample_ids, observ_ids].values.T\n",
    "print(_outname_, _data_.shape, subset_abundances.shape)\n",
    "table = Table(_data_, observ_ids, sample_ids, observ_metadata, None)\n",
    "with biom_open('../otu_data/FAPROTAX_out/otu_taxa{}.biom'.format(_outname_), 'w') as f:  \n",
    "    table.to_hdf5(f, \"faith and trust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtdb_rows = []\n",
    "largest_row = 0\n",
    "with open(\"/Volumes/KeithSSD/CB_V4/otu_data/FAPROTAX_out/fixrank_gtdb_classifiedx.txt\", 'r') as fh:\n",
    "    for l in fh:\n",
    "        gtdb_rows.append([i.strip() for i in l.split(\";\")])\n",
    "        if len(gtdb_rows[-1]) > largest_row:\n",
    "            largest_row = len(gtdb_rows[-1])\n",
    "\n",
    "print(len(gtdb_rows), largest_row)\n",
    "\n",
    "gtdb_fix_width = [i+[\"\"]*(largest_row-len(i)) for i in gtdb_rows]\n",
    "gtdb_taxa = pd.DataFrame(gtdb_fix_width).set_index(0)\n",
    "fix_conf = lambda x: int(x.split(\", \")[-1][:-2]) if str(x).startswith(\"(\") else 0\n",
    "\n",
    "gtdb_taxa.drop([1,2], inplace=True, axis=1)\n",
    "\n",
    "for num_col in range(2,8):\n",
    "    print(num_col*2)\n",
    "    gtdb_taxa[num_col*2] = gtdb_taxa[num_col*2].apply(fix_conf)\n",
    "    gtdb_taxa.loc[gtdb_taxa[num_col*2] < 60, (num_col*2)-1] = np.nan\n",
    "    gtdb_taxa.drop(num_col*2, axis=1, inplace=True)\n",
    "\n",
    "_outname_ = '_gtdb'\n",
    "observ_ids, observ_metadata = [], []\n",
    "for i in list(subset_abundances.columns):\n",
    "    if i.startswith(\"OTU\") and i in list(gtdb_taxa.index):\n",
    "        observ_ids.append(i)\n",
    "        observ_metadata.append({'taxonomy': [name_corrector(j) for j in gtdb_taxa.loc[i, :].dropna().values]})\n",
    "\n",
    "_data_ = subset_abundances.loc[sample_ids, observ_ids].values.T\n",
    "print(_outname_, _data_.shape, subset_abundances.shape)\n",
    "table = Table(_data_, observ_ids, sample_ids, observ_metadata, None)\n",
    "with biom_open('../otu_data/FAPROTAX_out/otu_taxa{}.biom'.format(_outname_), 'w') as f:  \n",
    "    table.to_hdf5(f, \"faith and trust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silva_fxn_df = pd.read_csv(\"/Volumes/KeithSSD/CB_V4/otu_data/FAPROTAX_out/report_silva.txt\", sep=\"\\t\",\n",
    "                 index_col=0, comment='#')\n",
    "rdp_fxn_df = pd.read_csv(\"/Volumes/KeithSSD/CB_V4/otu_data/FAPROTAX_out/report_rdp.txt\", sep=\"\\t\",\n",
    "                 index_col=0, comment='#')\n",
    "gtdb_fxn_df = pd.read_csv(\"/Volumes/KeithSSD/CB_V4/otu_data/FAPROTAX_out/report_gtdb.txt\", sep=\"\\t\",\n",
    "                 index_col=0, comment='#')\n",
    "\n",
    "print(gtdb_fxn_df.shape, rdp_fxn_df.shape, silva_fxn_df.shape)\n",
    "print(gtdb_fxn_df.sum().sum(), rdp_fxn_df.sum().sum(), silva_fxn_df.sum().sum())\n",
    "fxn_df = gtdb_fxn_df.astype(str) + rdp_fxn_df.astype(str) + silva_fxn_df.astype(str)\n",
    "print(np.unique(fxn_df.values, return_counts=True))\n",
    "\n",
    "fxn_df = ((gtdb_fxn_df + rdp_fxn_df + silva_fxn_df) > 0).astype(int).loc[subset_abundances.columns, :]\n",
    "fapro_raw = np.dot(subset_abundances.values, fxn_df.values) \n",
    "\n",
    "\n",
    "print(fapro_raw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fapro_df = pd.DataFrame(index=subset_abundances.index,\n",
    "                        columns=fxn_df.columns, \n",
    "                        data=fapro_raw)\n",
    "\n",
    "print(fapro_df.sum(1).shape)\n",
    "fapro_ra_df = fapro_df.div(fapro_df.sum(1), axis=0)*1e6\n",
    "\n",
    "fapro_ra_df.to_csv(\"/Users/login/Google Drive/SiYi_Xiaotong_Materials/FAPROTAX_counts.txt\", \n",
    "                sep='\\t', index_label='Sample')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
